# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/196IantqQHvNDsRSdtsrirFI4rZNuAUzx

# SYS ENG 6213 - Deep Learning and Advanced Neural Networks

In this assignment, you will implement the convolutional neural network and test it on a sample image. You can get good explanation of convolutional and pooling layers from the text book @ http://www.deeplearningbook.org/.


The code which you are to write will not be a __fast implementation__ since our goal is only to learn the convolution and pooling operation. The fast implementation can be implemented by converting convolution and pooling operations into matrix multiplications which is more complicated. 

Below is the architecture you will implement:

<img src="cnn_model.png" alt="Drawing" style="width: 200px;"/>

### 1. Python Implementation

complete the following functions in __layers.py__ and run the below code:

1. conv_forward (25 pts)
2. conv_backward (25 pts)
3. max_pool_forward (25 pts)
4. max_pool_backward (25 pts)
"""

# import necessary libraries
import tensorflow as tf
import datetime
import time
import numpy as np
import datetime
import matplotlib.pyplot as plt
# import necessary functions
from utilities import *
np.random.seed(231)

# load the data
maybe_download_and_extract()
train_,tr_target,val_,val_target = load_CIFAR10_data()

print('training data shape: ' + str(train_.shape))
print('training target shape: ' + str(tr_target.shape))
print('validation data shape: ' + str(val_.shape))
print('validation target shape: ' + str(val_target.shape))

#view a few examples
img_idx = np.random.choice(40000, 9, replace=False)
sample = train_[img_idx].reshape(9, 3, 32, 32).transpose(0,2,3,1).astype("uint8")
fig, axes1 = plt.subplots(3,3,figsize=(3,3))
i=0
for j in range(3):
    for k in range(3):
        #i = np.random.choice(range(len(sample)),replace=False)
        axes1[j][k].set_axis_off()
        axes1[j][k].imshow(sample[i])
        i=i+1
plt.show()

"""#### Try training on small data"""

# For CIFAR-10 we have 3-dimensional data which are independent of each other. 
# Therefore, we have to normalize each dimension separately

X_train = train_.reshape(len(train_), 3, 32, 32).astype("float32")
X_val = val_.reshape(len(val_), 3, 32, 32).astype("float32")
_means = [] # for future re-use
for i in range(3):
    _mean = np.mean(X_train[:,i,:,:]) 
    X_train[:,i,:,:] = (X_train[:,i,:,:] - _mean)
    X_val[:,i,:,:] = (X_val[:,i,:,:] - _mean)
    _means.append(_mean)
    

Y_train = tr_target
Y_val = val_target
n = 100
data = {'X_train':X_train[0:n],'Y_train':Y_train[0:n],'X_val':X_val[0:30],'Y_val':Y_val[0:30]}
# data = {'X_train':X_train,'Y_train':Y_train,'X_val':X_val,'Y_val':Y_val}

# Initialize two_layered_NN instance
from cnn_model import n_layered_NN
model = n_layered_NN(data,batch_size=30,epochs=5)
# Train the two layered neural network
start_time = datetime.datetime.now()
params,loss_history,train_acc_history,val_acc_history=model.train()
print(datetime.datetime.now() - start_time)

