# -*- coding: utf-8 -*-
"""hw8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bdfjHclUovohh_IrSvneD3s7MVBbmXLC

# SYS ENG 6213 - Deep Learning and Advanced Neural Networks

### Homework#5: RNN and LSTM networks

In this homework, you will be implementing the RNN and LSTM networks on Penn Treebank dataset. These architectures are useful for data which have dependencies like sequences and lists. For example, Language modelling where the future words in text are predicted based on history of previous words.

Few more applications:
1. Speech recognition 
2. Language modeling 
3. Translation 
4. Image captioning

Detailed information on RNN and LSTM can be found @ http://colah.github.io/posts/2015-08-Understanding-LSTMs/
"""

import os
import pickle as pickle
import reader
import tensorflow as tf
import numpy as np
from rnn_layers import *
from utilities import _get_batch,_divide_into_batches
from rnn import *

"""__Dataset:__

For this homework, we will use the [Penn Tree Bank](https://catalog.ldc.upenn.edu/ldc99t42) (PTB) dataset, which is a popular benchmark for measuring the quality of these models, whilst being small and relatively fast to train. Below is a small sample of data.
"""

with tf.io.gfile.GFile(os.path.join(os.getcwd(),'data', "ptb.train.txt"), "r") as f:
    data = f.read()
data[0:2000]

"""__Pre Processing__

The given dataset is a collection of words as shown above. In order to train an neural network we need numerical representation of the data. For this purpose, we encode each word in the dataset with unique numerical values. The following code implements the conversion of dataset words into numerical values.
"""

data_path = os.path.join(os.getcwd(),'data')
raw_data = reader.ptb_raw_data(data_path)
train_data, valid_data, test_data, vocabulary, word_ids = raw_data
data = {'train_data':train_data[0:10000],'valid_data':valid_data,'test_data':test_data,'vocabulary':vocabulary,'word_ids':word_ids}
ids_words = {i: w for w, i in word_ids.items()}

word_ids

"""### Demo for Recurrent Neural Network

<img src="rnn_pic.png" alt="Drawing" style="width: 600px;"/>

From the above figure you can observe that at RNN cell (left) is just a single neural network that is connected to itself. The unfolded representation is on right. It implies, at each time step the architecutre will not just recieve the current input but also the previous output. Therefore, the prediction at any time step depends on current input and all the previous inputs. This give the RNN the capability to learn a sequence.
"""

# model = language_model(data,update_rule='SGD_with_momentum',cell_type = 'rnn',batch_size = 128,seq_len= 2,epochs=70)
# params = model.train()
#
# """### Do some predictions"""
#
# # predict n next words
# n = 3
# model = language_model(data,cell_type = 'rnn',batch_size = 1,seq_len= n,use_pre_trained=True,params=params)
# k = np.random.choice(5000, 1)[0] #random point
# some_data = _divide_into_batches(train_data[k:k+100],1)
# pred_str = []
# for i in range(10):
#     x,_=_get_batch(some_data,i,n)
#     scores,_ = model.forward_pass(x)
#     predictions = np.argmax(scores, axis = 2)
#     #print(predictions)
#     print('-'*80)
#     print('actual sequnce    : '+str([ids_words[i] for i in x[0]]))
#     print('predicted sequence: '+str([ids_words[i] for i in predictions[0]]))

"""The predictions do not make any sense as we trained the model for only one epoch on a fraction of data.

### Implementation of Long Short Term Memory

The problem with RNN is that it is not suitable for learning long sequences. This inability is due to either vanishing gradient or exploding gradient. Fig below shows the examples of both. 

In case of vanishing gradient, when the sequence is long, the gradinent reduces at each step eventually becoming 0 (no more learning!). 

In case of exploding gradient, when the sequence is long, the gradient increases at each step eventually making the architecture unstable. 

To avoid both the scenarios, LSTM models were designed. The LSTM models use gates to decide which of the previous steps should participate in the current step prediction.

The LSTM cell is shown below:

<img src="lstm_pic.png" alt="Drawing" style="width: 600px;"/>

Complete the following functions in rnn_layers.py to implement LSTM architecture

1. lstm_forward
2. lstm_step_forward
3. lstm_backward
4. lstm_step_backward

Run the below code after implementing the functions
"""

model = language_model(data,update_rule='SGD_with_momentum',cell_type = 'lstm',batch_size = 128)
params = model.train()