{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "HW2_Sayantan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1mUj8b19lpV",
        "colab_type": "text"
      },
      "source": [
        "# SYS ENG 6213 - Deep Learning and Advanced Neural Networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agd8XtI9lpX",
        "colab_type": "text"
      },
      "source": [
        "### Implementation of two layer neural network for classification of MNIST data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMUjfyHu944Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4130f59a-a657-40d7-eeff-6cf8c36da6b9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd_avEPG-CnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b9422626-41a5-45b9-968d-72d6e73742f3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/3wH4bGg4sprm0sc0JaFXG284v0FxpedlLzEm4HToH99aqcijKfopjAw\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeS0X4kP-Ead",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e83f756d-db13-44f5-bf8b-042ef5048e98"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab Notebooks/HW2/\n",
        "!ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hw2_guide.pdf  HW2_Sayantan.ipynb  layers.py  MNIST_data  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qht_Pbuw9lpY",
        "colab_type": "text"
      },
      "source": [
        "In this homework you will be implementing a simple two layered neural network to recognize hand-written digits. Open the layers.py file. It has different functions which will be used for training the neural network. You need to complete each function with appropriate code in the spaces provided. You can check the correctness of your code by using the function calls below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1uoP4-9Hj3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load /content/drive/My Drive/Colab Notebooks/HW2/layers.py"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGOgTIqC9lpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the functions\n",
        "from layers import *\n",
        "from utils import *\n",
        "# Set up the packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-wV-UDBIYun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d87ecd33-1b8b-401b-ffaa-2837ad566626"
      },
      "source": [
        "print(softmax(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXovxjYT9lph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize values for check functions\n",
        "test_x = np.asarray([\n",
        "  [-0.000615  ,  0.0214419 , -0.36510585,  0.20082408],\n",
        "  [ 0.22389826,  0.14115274,  0.05578092,  0.1698094 ],\n",
        "  [-0.23772221,  0.00885403, -0.13013507,  0.02939912],\n",
        "  [ 0.05740505, -0.026481  ,  0.09055714, -0.51067097],\n",
        "  [ 0.02905955, -0.26222096,  0.23401101, -0.32704291]])\n",
        "test_w = np.asarray([\n",
        "  [ 0.14083526, -0.00589104, -0.01267299],\n",
        "  [ 0.2149024 ,  0.05601603,  0.22214587],\n",
        "  [-0.0246409 ,  0.01524164, -0.16539496],\n",
        "  [-0.07199102, -0.12796543,  0.2509505 ]])\n",
        "test_b = np.asarray([0.24348208,  0.02070011, -0.05303671])\n",
        "fs_scores = np.asarray([[ 0.24254239, -0.00935853,  0.06251788],\n",
        "       [ 0.29174967,  0.00640839,  0.0088702 ],\n",
        "       [ 0.21299534,  0.01685097, -0.01915576],\n",
        "       [ 0.28040822,  0.08560704, -0.20277768],\n",
        "       [ 0.20900069,  0.05125724, -0.23243211]])\n",
        "ftest_w = np.asarray([[ 0.03670967,  0.00383852, -0.01189351],\n",
        "        [-0.01396215, -0.01485459,  0.06874128],\n",
        "        [-0.02569631,  0.0213285 , -0.09259266],\n",
        "        [-0.10703647, -0.06077616,  0.19306612]])\n",
        "ftest_b = np.asarray([ 1.23669631,  0.15076511, -0.38297747])\n",
        "ftest_x = np.asarray([[ 0.03342136,  0.0654868 , -0.01645924, -0.00057441],\n",
        "        [ 0.04093848,  0.06502716, -0.00855839, -0.01959743],\n",
        "        [ 0.03014075,  0.04246176, -0.00182329, -0.02229724],\n",
        "        [ 0.04155685,  0.02000954,  0.02793369, -0.08202878],\n",
        "        [ 0.03207832, -0.00384786,  0.03407438, -0.07993428]])\n",
        "frelu = np.asarray([[ 0.24254239,  0.        ,  0.06251788],\n",
        "        [ 0.29174967,  0.00640839,  0.0088702 ],\n",
        "        [ 0.21299534,  0.01685097,  0.        ],\n",
        "        [ 0.28040822,  0.08560704,  0.        ],\n",
        "        [ 0.20900069,  0.05125724,  0.        ]])\n",
        "brelu = np.asarray([[ 0.14254239,  0.        , -0.03748212],\n",
        "       [ 0.19174967, -0.09359161, -0.0911298 ],\n",
        "       [ 0.11299534, -0.08314903,  0.        ],\n",
        "       [ 0.18040822, -0.01439296,  0.        ],\n",
        "       [ 0.10900069, -0.04874276,  0.        ]])\n",
        "ssoftmax = np.asarray([[ 0.25380621,  0.25946659,  0.17628115,  0.31044604],\n",
        "        [ 0.26931036,  0.24792319,  0.22763584,  0.25513061],\n",
        "        [ 0.21279206,  0.27229653,  0.23696266,  0.27794875],\n",
        "        [ 0.28410234,  0.26124234,  0.29367879,  0.16097652],\n",
        "        [ 0.27209608,  0.20333904,  0.33398855,  0.19057633]])\n",
        "test_y = np.asarray([3, 0, 1, 2, 1])\n",
        "test_l = 1.3201297629287778\n",
        "test_de = np.asarray([[ 0.05076124,  0.05189332,  0.03525623, -0.13791079],\n",
        "        [-0.14613793,  0.04958464,  0.04552717,  0.05102612],\n",
        "        [ 0.04255841, -0.14554069,  0.04739253,  0.05558975],\n",
        "        [ 0.05682047,  0.05224847, -0.14126424,  0.0321953 ],\n",
        "        [ 0.05441922, -0.15933219,  0.06679771,  0.03811527]])\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAPXzAKV9lpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "3fd26107-4d61-4ece-f3b8-010e9e587036"
      },
      "source": [
        "# Run this block to check the values for all functions in layers.py\n",
        "print('Note: The difference between the actual values and calculated values should be <1e-7 for all functions.')\n",
        "print('')\n",
        "print('1. Checking forward_step..')\n",
        "forward_step_scores, cache = forward_step(test_x, test_w, test_b)\n",
        "diff = np.sum(np.abs(forward_step_scores - fs_scores))\n",
        "print('The difference values is: '+str(diff))\n",
        "if diff < 1e-7:\n",
        "    print('forward_step check passed ...')\n",
        "else:\n",
        "    print('forward_step check failed !!')\n",
        "print('')\n",
        "print ('2. Checking backward_step...')\n",
        "dtest_w, dtest_b, dtest_x = backward_step(fs_scores, cache)\n",
        "diff_x = np.sum(np.abs(dtest_x - ftest_x))\n",
        "diff_w = np.sum(np.abs(dtest_w - ftest_w))\n",
        "diff_b = np.sum(np.abs(dtest_b - ftest_b))\n",
        "print('The difference values for dx is: ' + str(diff_x))\n",
        "print('The difference values for dw is: ' + str(diff_w))\n",
        "print('The difference values for db is: ' + str(diff_b))\n",
        "if (diff_x < 1e-7 and diff_w < 1e-7 and diff_b < 1e-7):\n",
        "    print('backward_step check passed !!')\n",
        "else:\n",
        "    print('backward_step check failed !!')\n",
        "print('')\n",
        "print('3. Checking ReLu_forward...')\n",
        "r_score, cache = ReLu_forward(fs_scores)\n",
        "diff_relu = np.sum(np.abs(r_score - frelu))\n",
        "print('The difference value is: ' + str(diff_relu))\n",
        "if diff_relu < 1e-7:\n",
        "    print('ReLu_forward check passed !!')\n",
        "else:\n",
        "    print('ReLu_forward check failed !!')\n",
        "print('')\n",
        "print('4. Checking ReLu_backward...')\n",
        "dr_score = ReLu_backward(frelu - 0.1, cache)\n",
        "diff_relu = np.sum(np.abs(dr_score - brelu))\n",
        "print('The difference value is: ' + str(diff_relu))\n",
        "if diff_relu < 1e-7:\n",
        "    print('ReLu_backward check passed !!')\n",
        "else:\n",
        "    print('ReLu_backward check failed !!')\n",
        "print('')\n",
        "print('5. Checking softmax...')\n",
        "s_score = softmax(test_x)\n",
        "diff = np.sum(np.abs(s_score - ssoftmax))\n",
        "print('The difference values is: ' + str(diff))\n",
        "if diff < 1e-7:\n",
        "    print('softmax check passed !!')\n",
        "else:\n",
        "    print('softmax check failed !!')\n",
        "print('')\n",
        "print('6. Checking loss...')\n",
        "l, lde = loss(ssoftmax, test_y)\n",
        "diff_l = np.sum(np.abs(l - test_l))\n",
        "diff_de = np.sum(np.abs(lde - test_de))\n",
        "print('The difference between values for loss is: ' + str(diff_l))\n",
        "print('The difference between values for de is:' + str(diff_de))\n",
        "if (diff_l < 1e-7 and diff_de < 1e-7):\n",
        "    print('loss check passed !!')\n",
        "else:\n",
        "    print('loss check failed !!')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: The difference between the actual values and calculated values should be <1e-7 for all functions.\n",
            "\n",
            "1. Checking forward_step..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c6c5391de07d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1. Checking forward_step..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mforward_step_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_step_scores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfs_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The difference values is: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2sNqIJgp9lpv",
        "colab_type": "text"
      },
      "source": [
        "If all the checks are passed, we can now start the training of the neural network for the MNIST dataset. We start by loading the data and displaying few images with their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guor4Pps9lpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the training data\n",
        "inputs, labels = load_images_with_labels()\n",
        "\n",
        "# View first 8 examples\n",
        "fig, ax = plt.subplots(1,8)\n",
        "labl = []\n",
        "for i in range(8):\n",
        "    ax[i].imshow(inputs[i], cmap=mpl.cm.Greys)\n",
        "    ax[i].set_title(labels[i])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOeyZEQ19lp2",
        "colab_type": "text"
      },
      "source": [
        "Now you have the training data and corresponding labels. Each image in training data is of shape (28,28) with values ranging from 0 to 255. However, neural network cannot take data in square format. Therefore, the preprocessing includes converting each image to shape (1,784) and normalizing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghMNSdI39lp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing the data\n",
        "train = inputs.reshape(60000, 784) # reshape the inputs shape from (60000,28,28) to (60000,784)\n",
        "train = np.float32(train) # change the datatype to float\n",
        "train /= np.max(train, axis=1).reshape(-1, 1) # Normalize the data between 0 and 1\n",
        "\n",
        "# Now we separate the inputs into training and validation\n",
        "train_ = train[0:50000,:] # We use first 50000 images for training\n",
        "tr_labels = labels[0: 50000]\n",
        "val = train[50000: 60000,:] # We use the last 10000 images for validation\n",
        "val_labels = labels[50000: 60000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMeP27Qg9lp8",
        "colab_type": "text"
      },
      "source": [
        "In order to simplify the training, we now define a function for forward pass which will be used in training the neural network and predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL3AuGpg9lp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(x, w1, b1, w2, b2):\n",
        "    \"\"\"\n",
        "    TO DO: Compute the forward pass of the neural network\n",
        "    Points Allocated: 5\n",
        "    \n",
        "    Inputs:\n",
        "    x: Numpy array of shape (N,d) where N is number of samples \n",
        "    and d is the dimension of input\n",
        "    w1: numpy array of shape (d,H) where H is the size of hidden layer\n",
        "    w2: numpy array of shape (H,c) where c is the number of classes\n",
        "    b1: numpy array of shape (H,) \n",
        "    b2: numpy array of shape (c,)\n",
        "    \n",
        "    Outputs:\n",
        "    probs: output of shape (N,c)\n",
        "    cache_out: cache values for output layer\n",
        "    cache_relu: cache values for ReLu layer\n",
        "    cache_ip: cache values for input layer\n",
        "    \"\"\"\n",
        "    probs, cache_out, cache_relu, cache_ip = None, None, None, None\n",
        "    \n",
        "    ### Type your code here ###\n",
        "    fs_scores, cache_ip = forward_step(x, w1, b1)\n",
        "    relu_score, cache_relu = ReLu_forward(fs_scores)\n",
        "    fs_scores, cache_out = forward_step(relu_score, w2, b2)\n",
        "    probs = softmax(fs_scores)\n",
        "    #### End of your code #### \n",
        "    \n",
        "    return(probs,cache_out,cache_relu,cache_ip)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmklDOud9lqD",
        "colab_type": "text"
      },
      "source": [
        "Now that you have implemented forward pass, the stochastic gradient descent algorithm works by finding the loss/error in forward pass with respect to target and traversing the error backwards. In the following function, you will implement the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKTJ2LQ_9lqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_pass(probs, y, cache_out, cache_relu, cache_ip):\n",
        "    \"\"\"\n",
        "    TO DO: Compute the backward pass of the neural network\n",
        "    Points Allocated: 5\n",
        "    \n",
        "    Inputs:\n",
        "    probs: output of shape (N,c)\n",
        "    cache_out: cache values for output layer\n",
        "    cache_relu: cache values for ReLu layer\n",
        "    cache_ip: cache values for input layer\n",
        "    \n",
        "    Outputs:\n",
        "    loss_: loss value of the forward pass\n",
        "    dw2: numpy array with samw shape as w2\n",
        "    db2: numpy array with same shape as b2\n",
        "    dw1: numpy array with same shape as w1\n",
        "    db1: numpy array with same shape as b1\n",
        "    \"\"\"\n",
        "    ### Type your code here ###\n",
        "    loss_, dw_softmax = loss(probs, y)\n",
        "    dw2, db2, dx = backward_step(dw_softmax, cache_out)\n",
        "    dw_relu = ReLu_backward(dx, cache_relu)\n",
        "    dw1, db1, dx = backward_step(dw_relu, cache_ip)\n",
        "    db1, db2 = db1[0], db2[0]  # converted (1, 30) shape to (30,)    \n",
        "    #### End of your code #### \n",
        "    \n",
        "    return(loss_,dw2,db2,dw1,db1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAr4UGu39lqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following function will be used to predict the labels for given images, weights and biases\n",
        "def predict(X_batch,parameters):\n",
        "    probs, _, _, _ = forward_pass(X_batch, parameters['w1'], parameters['b1'], parameters['w2'], parameters['b2'])\n",
        "    y_pred = np.argmax(probs, axis = 1)\n",
        "    return (y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aLVButd9lqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {}\n",
        "w1 = 0.3 * np.random.randn(784, 30)\n",
        "b1 = np.zeros(30)\n",
        "w2 = 0.3 * np.random.randn(30, 10)\n",
        "b2 = np.zeros(10)\n",
        "grads={}\n",
        "loss_,grads['w1'], grads['b1'], grads['w2'], grads['b2'] = None, None, None, None, None\n",
        "val_size = 10000\n",
        "loss_history = []\n",
        "idx = np.random.choice(50000, 100, replace=True)\n",
        "x = train[idx]\n",
        "y = labels[idx]\n",
        "hi,cache_ip = forward_step(x,w1,b1)\n",
        "ho, cache_relu = ReLu_forward(hi)\n",
        "out, cache_out = forward_step(ho,w2,b2)\n",
        "probs = softmax(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIIQWCaE9lqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLayerNN(learning_rate, num_iters, batch_size, train, labels, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Function to train the two layered neural network to predict MNIST data.\n",
        "    Inputs: \n",
        "    learning_rate: scalar value contating learning rate for training\n",
        "    num_iters: number of iterations for training\n",
        "    batch_size: number of sample used for training in each iteration\n",
        "    train: trainig data\n",
        "    labels: labels fro the training data\n",
        "    X_val: inputs for validation data\n",
        "    y_val: labels for validation data\n",
        "    \n",
        "    Output: \n",
        "    parameters: dictionary containing trained weights and biases\n",
        "    loss_history: list contating loss values for each iteration during training. It will have length of num_iters\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    parameters['w1'] = 0.3 * np.random.randn(784, 30)\n",
        "    parameters['b1'] = np.zeros(30)\n",
        "    parameters['w2'] = 0.3 * np.random.randn(30, 10)\n",
        "    parameters['b2'] = np.zeros(10)\n",
        "    grads={}\n",
        "    loss_, grads['w1'], grads['b1'], grads['w2'], grads['b2'] = None, None, None, None, None\n",
        "    val_size = 10000\n",
        "    loss_history = []\n",
        "    for it in range(num_iters):\n",
        "        idx = np.random.choice(50000, batch_size, replace=True)\n",
        "        X_batch = train[idx]\n",
        "        y_batch = labels[idx]\n",
        "        # The following steps implement the forward and backward pass\n",
        "        probs, cache_out, cache_relu, cache_ip = forward_pass(X_batch, parameters['w1'], parameters['b1'], parameters['w2'], parameters['b2'])\n",
        "        loss_, grads['w2'], grads['b2'], grads['w1'], grads['b1'] = backward_pass(probs, y_batch, cache_out, cache_relu, cache_ip)\n",
        "        loss_history.append(loss_)\n",
        "        #Now update the weights and biases in paramaters\n",
        "        \n",
        "        ### Type your code here ###\n",
        "        parameters['w1'] += -learning_rate * grads['w1']\n",
        "        parameters['b1'] += -learning_rate * grads['b1']\n",
        "        parameters['w2'] += -learning_rate * grads['w2']\n",
        "        parameters['b2'] += -learning_rate * grads['b2']\n",
        "        #### End of your code #### \n",
        "        \n",
        "        train_acc = (predict(X_batch, parameters) == y_batch).mean()\n",
        "        val_acc = (predict(X_val, parameters) == y_val).mean()\n",
        "        \n",
        "        if it % 10 == 0:\n",
        "            print ('iteration '+str(it) + ' / '+ str(num_iters) +' :loss ' + str(loss_))\n",
        "            print('training accuracy: '+ str(train_acc) + ' and validation accuracy: '+ str(val_acc))\n",
        "    return (parameters,loss_history)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JQ2A1Wx9lqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters, loss_history = TwoLayerNN(0.1, 1000, 200, train_, tr_labels, val, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y729JQy9lqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the loss to see how the loss varied over iterations\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.title('Training Loss history')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvurG86T9lqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading and pre=processing test data\n",
        "t_inputs, tlabels = load_images_with_labels(type_of_data='testing')\n",
        "tinputs = t_inputs.reshape(10000, 784)\n",
        "tinputs = np.float32(tinputs)\n",
        "tinputs /= np.max(tinputs,axis=1).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyJO0P99lqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the accuracy on test data using trained weights and biases\n",
        "pred = predict(tinputs, parameters)\n",
        "test_acc = (pred == tlabels).mean()\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQjF7n939lqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict the labels for n images for visual representation\n",
        "n = 5 # number of images to predict\n",
        "idx = np.random.choice(10000, n, replace=False) # select n random images from test data\n",
        "labl=[]\n",
        "# View first n examples\n",
        "fig, ax = plt.subplots(1,n)\n",
        "for i, val in enumerate(idx):\n",
        "    ax[i].imshow(t_inputs[val], cmap=mpl.cm.Greys)\n",
        "    ax[i].set_title(tlabels[val])\n",
        "    labl.append(pred[val])\n",
        "plt.show()\n",
        "\n",
        "print('Corresponding predictions of labels for each of the above images: '+ str(labl))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}