# -*- coding: utf-8 -*-
"""HW2_Sayantan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1quTu9Vv1Asz0cWGGnjBWld62TUAKwMOc

# SYS ENG 6213 - Deep Learning and Advanced Neural Networks

### Implementation of two layer neural network for classification of MNIST data
"""

"""In this homework you will be implementing a simple two layered neural network to recognize hand-written digits. Open the layers.py file. It has different functions which will be used for training the neural network. You need to complete each function with appropriate code in the spaces provided. You can check the correctness of your code by using the function calls below:"""

# Import the functions
from layers import *
from utils import *
# Set up the packages
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

"""Now you have the training data and corresponding labels. Each image in training data is of shape (28,28) with values ranging from 0 to 255. However, neural network cannot take data in square format. Therefore, the preprocessing includes converting each image to shape (1,784) and normalizing them."""

"""In order to simplify the training, we now define a function for forward pass which will be used in training the neural network and predicting."""


def forward_pass(x, w1, b1, w2, b2):
    """
    TO DO: Compute the forward pass of the neural network
    Points Allocated: 5

    Inputs:
    x: Numpy array of shape (N,d) where N is number of samples
    and d is the dimension of input
    w1: numpy array of shape (d,H) where H is the size of hidden layer
    w2: numpy array of shape (H,c) where c is the number of classes
    b1: numpy array of shape (H,)
    b2: numpy array of shape (c,)

    Outputs:
    probs: output of shape (N,c)
    cache_out: cache values for output layer
    cache_relu: cache values for ReLu layer
    cache_ip: cache values for input layer
    """
    probs, cache_out, cache_relu, cache_ip = None, None, None, None

    ### Type your code here ###
    fs_scores, cache_ip = forward_step(x, w1, b1)
    relu_score, cache_relu = ReLu_forward(fs_scores)
    fs_scores, cache_out = forward_step(relu_score, w2, b2)
    probs = softmax(fs_scores)
    #### End of your code ####
    # print('\nForward pass')
    # print(probs, cache_out, cache_relu, cache_ip)

    return(probs, cache_out, cache_relu, cache_ip)

"""Now that you have implemented forward pass, the stochastic gradient descent algorithm works by finding the loss/error in forward pass with respect to target and traversing the error backwards. In the following function, you will implement the backward pass."""


def backward_pass(probs, y, cache_out, cache_relu, cache_ip):
    """
    TO DO: Compute the backward pass of the neural network
    Points Allocated: 5

    Inputs:
    probs: output of shape (N,c)
    cache_out: cache values for output layer
    cache_relu: cache values for ReLu layer
    cache_ip: cache values for input layer

    Outputs:
    loss_: loss value of the forward pass
    dw2: numpy array with same shape as w2
    db2: numpy array with same shape as b2
    dw1: numpy array with same shape as w1
    db1: numpy array with same shape as b1
    """
    ### Type your code here ###

    loss_, dw_softmax = loss(probs, y)
    # print('\nBackward pass')
    # print('Output layer')
    dw2, db2, dx = backward_step(dw_softmax, cache_out)
    # print(loss_, dw_softmax, cache_out)
    # print('Others')
    # print(dw2, db2, dx)
    # print('Hidden layer')
    dw_relu = ReLu_backward(dx, cache_relu)
    # print(dw_relu, cache_relu, cache_ip)
    dw1, db1, dx = backward_step(dw_relu, cache_ip, input_layer=False)
    db1, db2 = db1.T, db2.T  # converted (1, 30) shape to (30,)
    ### End of your code ####
    # print('Input layer')
    # print(dw1, db1)

    return(loss_, dw2, db2, dw1, db1)

# The following function will be used to predict the labels for given images, weights and biases


def predict(X_batch,parameters):
    probs, _, _, _ = forward_pass(X_batch, parameters['w1'], parameters['b1'], parameters['w2'], parameters['b2'])
    y_pred = np.argmax(probs, axis=1)
    return (y_pred)


def TwoLayerNN(learning_rate, num_iters, batch_size=None, train=None, labels=None, X_val=None, y_val=None):
    """
    Function to train the two layered neural network to predict MNIST data.
    Inputs:
    learning_rate: scalar value contating learning rate for training
    num_iters: number of iterations for training
    batch_size: number of sample used for training in each iteration
    train: trainig data
    labels: labels fro the training data
    X_val: inputs for validation data
    y_val: labels for validation data

    Output:
    parameters: dictionary containing trained weights and biases
    loss_history: list contating loss values for each iteration during training. It will have length of num_iters
    """

    parameters = {}
    parameters['w1'] = np.array([[0.24, 0.67], [0.43, 0.14]])
    parameters['b1'] = np.array([[1.], [1.]])
    parameters['w2'] = np.array([[0.34, 0.63], [0.81, 0.72]])
    parameters['b2'] = np.array([[1.], [1.]])
    grads={}
    loss_, grads['w1'], grads['b1'], grads['w2'], grads['b2'] = None, None, None, None, None
    val_size = 10000
    loss_history = []
    X = np.array([0.8, 0.3])
    y = np.array([0, 1])
    for it in range(num_iters):
        # The following steps implement the forward and backward pass
        probs, cache_out, cache_relu, cache_ip = forward_pass(X, parameters['w1'], parameters['b1'], parameters['w2'],
                                                              parameters['b2'])
        loss_, grads['w2'], grads['b2'], grads['w1'], grads['b1'] = backward_pass(probs, y, cache_out, cache_relu, cache_ip)
        loss_history.append(loss_)
        #Now update the weights and biases in paramaters


        ### Type your code here ###
        parameters['w1'] += -learning_rate * grads['w1']
        parameters['b1'] += -learning_rate * grads['b1'].reshape(2, 1)
        parameters['w2'] += -learning_rate * grads['w2']
        parameters['b2'] += -learning_rate * np.array(grads['b2']).reshape(2, 1)
        #### End of your code ####

        train_acc = (predict(X, parameters) == y).mean()
        val_acc = (predict(X, parameters) == y).mean()

        if it % 10 == 0:
            print ('iteration '+str(it) + ' / '+ str(num_iters) +' :loss ' + str(loss_))
            print('training accuracy: '+ str(train_acc) + ' and validation accuracy: '+ str(val_acc))
    return (parameters,loss_history)


parameters, loss_history = TwoLayerNN(1, 200)
# print('\nFinal parameters\n', parameters)