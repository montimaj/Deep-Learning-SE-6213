{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "HW2_Sayantan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/montimaj/Deep-Learning-SE-6213/blob/master/HW2/HW2_Sayantan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1mUj8b19lpV",
        "colab_type": "text"
      },
      "source": [
        "# SYS ENG 6213 - Deep Learning and Advanced Neural Networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agd8XtI9lpX",
        "colab_type": "text"
      },
      "source": [
        "### Implementation of two layer neural network for classification of MNIST data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMUjfyHu944Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcf88899-796b-434c-f29b-3f6fa3108e02"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd_avEPG-CnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "cc920be8-79f9-46f8-9acf-1916ffe5a30b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/3wGmt-G1CpZDP7cscan3i2lW_1joRn6LCVsePqNb7AGaBgI5sY45yl8\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeS0X4kP-Ead",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "87b59c76-46a8-4b70-faa0-db8ed824cc01"
      },
      "source": [
        "%cd 'drive/My Drive/SysEng 6213 Fall 2020 Sayantan Majumdar /HW2/'\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1xpWpUfn0NqGtbgAP_303jr6QW5RKzJPR/SysEng 6213 Fall 2020 Sayantan Majumdar /HW2\n",
            "HW2_Sayantan.ipynb  layers.py\tMNIST_data\n",
            "hw2_sayantan.py     layers.pyc\tutils.pyc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qht_Pbuw9lpY",
        "colab_type": "text"
      },
      "source": [
        "In this homework you will be implementing a simple two layered neural network to recognize hand-written digits. Open the layers.py file. It has different functions which will be used for training the neural network. You need to complete each function with appropriate code in the spaces provided. You can check the correctness of your code by using the function calls below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGOgTIqC9lpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the functions\n",
        "from layers import *\n",
        "from utils import *\n",
        "# Set up the packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-wV-UDBIYun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0d81697f-e5fb-4214-ba50-1a00aee38733"
      },
      "source": [
        "# Test cell to check if user-defined functions from imported files work\n",
        "print(softmax(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]\n",
            " [0.09003057 0.24472847 0.66524096]\n",
            " [0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXovxjYT9lph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize values for check functions\n",
        "test_x = np.asarray([\n",
        "  [-0.000615  ,  0.0214419 , -0.36510585,  0.20082408],\n",
        "  [ 0.22389826,  0.14115274,  0.05578092,  0.1698094 ],\n",
        "  [-0.23772221,  0.00885403, -0.13013507,  0.02939912],\n",
        "  [ 0.05740505, -0.026481  ,  0.09055714, -0.51067097],\n",
        "  [ 0.02905955, -0.26222096,  0.23401101, -0.32704291]])\n",
        "test_w = np.asarray([\n",
        "  [ 0.14083526, -0.00589104, -0.01267299],\n",
        "  [ 0.2149024 ,  0.05601603,  0.22214587],\n",
        "  [-0.0246409 ,  0.01524164, -0.16539496],\n",
        "  [-0.07199102, -0.12796543,  0.2509505 ]])\n",
        "test_b = np.asarray([0.24348208,  0.02070011, -0.05303671])\n",
        "fs_scores = np.asarray([[ 0.24254239, -0.00935853,  0.06251788],\n",
        "       [ 0.29174967,  0.00640839,  0.0088702 ],\n",
        "       [ 0.21299534,  0.01685097, -0.01915576],\n",
        "       [ 0.28040822,  0.08560704, -0.20277768],\n",
        "       [ 0.20900069,  0.05125724, -0.23243211]])\n",
        "ftest_w = np.asarray([[ 0.03670967,  0.00383852, -0.01189351],\n",
        "        [-0.01396215, -0.01485459,  0.06874128],\n",
        "        [-0.02569631,  0.0213285 , -0.09259266],\n",
        "        [-0.10703647, -0.06077616,  0.19306612]])\n",
        "ftest_b = np.asarray([ 1.23669631,  0.15076511, -0.38297747])\n",
        "ftest_x = np.asarray([[ 0.03342136,  0.0654868 , -0.01645924, -0.00057441],\n",
        "        [ 0.04093848,  0.06502716, -0.00855839, -0.01959743],\n",
        "        [ 0.03014075,  0.04246176, -0.00182329, -0.02229724],\n",
        "        [ 0.04155685,  0.02000954,  0.02793369, -0.08202878],\n",
        "        [ 0.03207832, -0.00384786,  0.03407438, -0.07993428]])\n",
        "frelu = np.asarray([[ 0.24254239,  0.        ,  0.06251788],\n",
        "        [ 0.29174967,  0.00640839,  0.0088702 ],\n",
        "        [ 0.21299534,  0.01685097,  0.        ],\n",
        "        [ 0.28040822,  0.08560704,  0.        ],\n",
        "        [ 0.20900069,  0.05125724,  0.        ]])\n",
        "brelu = np.asarray([[ 0.14254239,  0.        , -0.03748212],\n",
        "       [ 0.19174967, -0.09359161, -0.0911298 ],\n",
        "       [ 0.11299534, -0.08314903,  0.        ],\n",
        "       [ 0.18040822, -0.01439296,  0.        ],\n",
        "       [ 0.10900069, -0.04874276,  0.        ]])\n",
        "ssoftmax = np.asarray([[ 0.25380621,  0.25946659,  0.17628115,  0.31044604],\n",
        "        [ 0.26931036,  0.24792319,  0.22763584,  0.25513061],\n",
        "        [ 0.21279206,  0.27229653,  0.23696266,  0.27794875],\n",
        "        [ 0.28410234,  0.26124234,  0.29367879,  0.16097652],\n",
        "        [ 0.27209608,  0.20333904,  0.33398855,  0.19057633]])\n",
        "test_y = np.asarray([3, 0, 1, 2, 1])\n",
        "test_l = 1.3201297629287778\n",
        "test_de = np.asarray([[ 0.05076124,  0.05189332,  0.03525623, -0.13791079],\n",
        "        [-0.14613793,  0.04958464,  0.04552717,  0.05102612],\n",
        "        [ 0.04255841, -0.14554069,  0.04739253,  0.05558975],\n",
        "        [ 0.05682047,  0.05224847, -0.14126424,  0.0321953 ],\n",
        "        [ 0.05441922, -0.15933219,  0.06679771,  0.03811527]])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAPXzAKV9lpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "76ce2988-ca45-4031-a173-7cab3dd57ba8"
      },
      "source": [
        "# Run this block to check the values for all functions in layers.py\n",
        "print('Note: The difference between the actual values and calculated values should be <1e-7 for all functions.')\n",
        "print('')\n",
        "print('1. Checking forward_step..')\n",
        "forward_step_scores, cache = forward_step(test_x, test_w, test_b)\n",
        "diff = np.sum(np.abs(forward_step_scores - fs_scores))\n",
        "print('The difference values is: '+str(diff))\n",
        "if diff < 1e-7:\n",
        "    print('forward_step check passed ...')\n",
        "else:\n",
        "    print('forward_step check failed !!')\n",
        "print('')\n",
        "print ('2. Checking backward_step...')\n",
        "dtest_w, dtest_b, dtest_x = backward_step(fs_scores, cache)\n",
        "diff_x = np.sum(np.abs(dtest_x - ftest_x))\n",
        "diff_w = np.sum(np.abs(dtest_w - ftest_w))\n",
        "diff_b = np.sum(np.abs(dtest_b - ftest_b))\n",
        "print('The difference values for dx is: ' + str(diff_x))\n",
        "print('The difference values for dw is: ' + str(diff_w))\n",
        "print('The difference values for db is: ' + str(diff_b))\n",
        "if (diff_x < 1e-7 and diff_w < 1e-7 and diff_b < 1e-7):\n",
        "    print('backward_step check passed !!')\n",
        "else:\n",
        "    print('backward_step check failed !!')\n",
        "print('')\n",
        "print('3. Checking ReLu_forward...')\n",
        "r_score, cache = ReLu_forward(fs_scores)\n",
        "diff_relu = np.sum(np.abs(r_score - frelu))\n",
        "print('The difference value is: ' + str(diff_relu))\n",
        "if diff_relu < 1e-7:\n",
        "    print('ReLu_forward check passed !!')\n",
        "else:\n",
        "    print('ReLu_forward check failed !!')\n",
        "print('')\n",
        "print('4. Checking ReLu_backward...')\n",
        "dr_score = ReLu_backward(frelu - 0.1, cache)\n",
        "diff_relu = np.sum(np.abs(dr_score - brelu))\n",
        "print('The difference value is: ' + str(diff_relu))\n",
        "if diff_relu < 1e-7:\n",
        "    print('ReLu_backward check passed !!')\n",
        "else:\n",
        "    print('ReLu_backward check failed !!')\n",
        "print('')\n",
        "print('5. Checking softmax...')\n",
        "s_score = softmax(test_x)\n",
        "diff = np.sum(np.abs(s_score - ssoftmax))\n",
        "print('The difference values is: ' + str(diff))\n",
        "if diff < 1e-7:\n",
        "    print('softmax check passed !!')\n",
        "else:\n",
        "    print('softmax check failed !!')\n",
        "print('')\n",
        "print('6. Checking loss...')\n",
        "l, lde = loss(ssoftmax, test_y)\n",
        "diff_l = np.sum(np.abs(l - test_l))\n",
        "diff_de = np.sum(np.abs(lde - test_de))\n",
        "print('The difference between values for loss is: ' + str(diff_l))\n",
        "print('The difference between values for de is:' + str(diff_de))\n",
        "if (diff_l < 1e-7 and diff_de < 1e-7):\n",
        "    print('loss check passed !!')\n",
        "else:\n",
        "    print('loss check failed !!')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: The difference between the actual values and calculated values should be <1e-7 for all functions.\n",
            "\n",
            "1. Checking forward_step..\n",
            "The difference values is: 3.690392374848528e-08\n",
            "forward_step check passed ...\n",
            "\n",
            "2. Checking backward_step...\n",
            "The difference values for dx is: 5.7273561981582843e-08\n",
            "The difference values for dw is: 3.372888388855286e-08\n",
            "The difference values for db is: 2.220446049250313e-16\n",
            "backward_step check passed !!\n",
            "\n",
            "3. Checking ReLu_forward...\n",
            "The difference value is: 0.0\n",
            "ReLu_forward check passed !!\n",
            "\n",
            "4. Checking ReLu_backward...\n",
            "The difference value is: 8.673617379884035e-17\n",
            "ReLu_backward check passed !!\n",
            "\n",
            "5. Checking softmax...\n",
            "The difference values is: 5.824567878010001e-08\n",
            "softmax check passed !!\n",
            "\n",
            "6. Checking loss...\n",
            "The difference between values for loss is: 0.0\n",
            "The difference between values for de is:4.199999995013748e-08\n",
            "loss check passed !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2sNqIJgp9lpv",
        "colab_type": "text"
      },
      "source": [
        "If all the checks are passed, we can now start the training of the neural network for the MNIST dataset. We start by loading the data and displaying few images with their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guor4Pps9lpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "e2a15169-b90d-4c91-bdd8-365acd464ef1"
      },
      "source": [
        "# Load the training data\n",
        "inputs, labels = load_images_with_labels()\n",
        "\n",
        "# View first 8 examples\n",
        "fig, ax = plt.subplots(1,8)\n",
        "labl = []\n",
        "for i in range(8):\n",
        "    ax[i].imshow(inputs[i], cmap=mpl.cm.Greys)\n",
        "    ax[i].set_title(labels[i])\n",
        "plt.show()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOeyZEQ19lp2",
        "colab_type": "text"
      },
      "source": [
        "Now you have the training data and corresponding labels. Each image in training data is of shape (28,28) with values ranging from 0 to 255. However, neural network cannot take data in square format. Therefore, the preprocessing includes converting each image to shape (1,784) and normalizing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghMNSdI39lp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing the data\n",
        "train = inputs.reshape(60000, 784) # reshape the inputs shape from (60000,28,28) to (60000,784)\n",
        "train = np.float32(train) # change the datatype to float\n",
        "train /= np.max(train, axis=1).reshape(-1, 1) # Normalize the data between 0 and 1\n",
        "\n",
        "# Now we separate the inputs into training and validation\n",
        "train_ = train[0: 50000, :] # We use first 50000 images for training\n",
        "tr_labels = labels[0: 50000]\n",
        "val = train[50000: 60000,:] # We use the last 10000 images for validation\n",
        "val_labels = labels[50000: 60000]\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMeP27Qg9lp8",
        "colab_type": "text"
      },
      "source": [
        "In order to simplify the training, we now define a function for forward pass which will be used in training the neural network and predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL3AuGpg9lp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(x, w1, b1, w2, b2):\n",
        "    \"\"\"\n",
        "    TO DO: Compute the forward pass of the neural network\n",
        "    Points Allocated: 5\n",
        "    \n",
        "    Inputs:\n",
        "    x: Numpy array of shape (N,d) where N is number of samples \n",
        "    and d is the dimension of input\n",
        "    w1: numpy array of shape (d,H) where H is the size of hidden layer\n",
        "    w2: numpy array of shape (H,c) where c is the number of classes\n",
        "    b1: numpy array of shape (H,) \n",
        "    b2: numpy array of shape (c,)\n",
        "    \n",
        "    Outputs:\n",
        "    probs: output of shape (N,c)\n",
        "    cache_out: cache values for output layer\n",
        "    cache_relu: cache values for ReLu layer\n",
        "    cache_ip: cache values for input layer\n",
        "    \"\"\"\n",
        "    probs, cache_out, cache_relu, cache_ip = None, None, None, None\n",
        "    \n",
        "    ### Type your code here ###\n",
        "    fs_scores, cache_ip = forward_step(x, w1, b1)\n",
        "    relu_score, cache_relu = ReLu_forward(fs_scores)\n",
        "    fs_scores, cache_out = forward_step(relu_score, w2, b2)\n",
        "    probs = softmax(fs_scores)\n",
        "    #### End of your code #### \n",
        "    \n",
        "    return(probs,cache_out,cache_relu,cache_ip)\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmklDOud9lqD",
        "colab_type": "text"
      },
      "source": [
        "Now that you have implemented forward pass, the stochastic gradient descent algorithm works by finding the loss/error in forward pass with respect to target and traversing the error backwards. In the following function, you will implement the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKTJ2LQ_9lqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_pass(probs, y, cache_out, cache_relu, cache_ip):\n",
        "    \"\"\"\n",
        "    TO DO: Compute the backward pass of the neural network\n",
        "    Points Allocated: 5\n",
        "    \n",
        "    Inputs:\n",
        "    probs: output of shape (N,c)\n",
        "    cache_out: cache values for output layer\n",
        "    cache_relu: cache values for ReLu layer\n",
        "    cache_ip: cache values for input layer\n",
        "    \n",
        "    Outputs:\n",
        "    loss_: loss value of the forward pass\n",
        "    dw2: numpy array with samw shape as w2\n",
        "    db2: numpy array with same shape as b2\n",
        "    dw1: numpy array with same shape as w1\n",
        "    db1: numpy array with same shape as b1\n",
        "    \"\"\"\n",
        "    ### Type your code here ###\n",
        "    loss_, dw_softmax = loss(probs, y)\n",
        "    dw2, db2, dx = backward_step(dw_softmax, cache_out)\n",
        "    dw_relu = ReLu_backward(dx, cache_relu)\n",
        "    dw1, db1, dx = backward_step(dw_relu, cache_ip)\n",
        "    db1, db2 = db1[0], db2[0]  # converted (1, 30) shape to (30,)    \n",
        "    #### End of your code #### \n",
        "    \n",
        "    return(loss_,dw2,db2,dw1,db1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAr4UGu39lqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following function will be used to predict the labels for given images, weights and biases\n",
        "def predict(X_batch,parameters):\n",
        "    probs, _, _, _ = forward_pass(X_batch, parameters['w1'], parameters['b1'], parameters['w2'], parameters['b2'])\n",
        "    y_pred = np.argmax(probs, axis = 1)\n",
        "    return (y_pred)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aLVButd9lqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {}\n",
        "w1 = 0.3 * np.random.randn(784, 30)\n",
        "b1 = np.zeros(30)\n",
        "w2 = 0.3 * np.random.randn(30, 10)\n",
        "b2 = np.zeros(10)\n",
        "grads={}\n",
        "loss_,grads['w1'], grads['b1'], grads['w2'], grads['b2'] = None, None, None, None, None\n",
        "val_size = 10000\n",
        "loss_history = []\n",
        "idx = np.random.choice(50000, 100, replace=True)\n",
        "x = train[idx]\n",
        "y = labels[idx]\n",
        "hi,cache_ip = forward_step(x,w1,b1)\n",
        "ho, cache_relu = ReLu_forward(hi)\n",
        "out, cache_out = forward_step(ho,w2,b2)\n",
        "probs = softmax(out)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIIQWCaE9lqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLayerNN(learning_rate, num_iters, batch_size, train, labels, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Function to train the two layered neural network to predict MNIST data.\n",
        "    Inputs: \n",
        "    learning_rate: scalar value contating learning rate for training\n",
        "    num_iters: number of iterations for training\n",
        "    batch_size: number of sample used for training in each iteration\n",
        "    train: trainig data\n",
        "    labels: labels fro the training data\n",
        "    X_val: inputs for validation data\n",
        "    y_val: labels for validation data\n",
        "    \n",
        "    Output: \n",
        "    parameters: dictionary containing trained weights and biases\n",
        "    loss_history: list contating loss values for each iteration during training. It will have length of num_iters\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    parameters['w1'] = 0.3 * np.random.randn(784, 30)\n",
        "    parameters['b1'] = np.zeros(30)\n",
        "    parameters['w2'] = 0.3 * np.random.randn(30, 10)\n",
        "    parameters['b2'] = np.zeros(10)\n",
        "    grads={}\n",
        "    loss_, grads['w1'], grads['b1'], grads['w2'], grads['b2'] = None, None, None, None, None\n",
        "    val_size = 10000\n",
        "    loss_history = []\n",
        "    for it in range(num_iters + 1):\n",
        "        idx = np.random.choice(50000, batch_size, replace=True)\n",
        "        X_batch = train[idx]\n",
        "        y_batch = labels[idx]\n",
        "        # The following steps implement the forward and backward pass\n",
        "        probs, cache_out, cache_relu, cache_ip = forward_pass(X_batch, parameters['w1'], parameters['b1'], parameters['w2'], parameters['b2'])\n",
        "        loss_, grads['w2'], grads['b2'], grads['w1'], grads['b1'] = backward_pass(probs, y_batch, cache_out, cache_relu, cache_ip)\n",
        "        loss_history.append(loss_)\n",
        "        #Now update the weights and biases in paramaters\n",
        "        \n",
        "        ### Type your code here ###\n",
        "        parameters['w1'] += -learning_rate * grads['w1']\n",
        "        parameters['b1'] += -learning_rate * grads['b1']\n",
        "        parameters['w2'] += -learning_rate * grads['w2']\n",
        "        parameters['b2'] += -learning_rate * grads['b2']\n",
        "        #### End of your code #### \n",
        "        \n",
        "        train_acc = (predict(X_batch, parameters) == y_batch).mean()\n",
        "        val_acc = (predict(X_val, parameters) == y_val).mean()\n",
        "        \n",
        "        if it % 10 == 0:\n",
        "            print ('iteration '+str(it) + ' / '+ str(num_iters) +' :loss ' + str(loss_))\n",
        "            print('training accuracy: '+ str(train_acc) + ' and validation accuracy: '+ str(val_acc))\n",
        "    return (parameters,loss_history)    "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPZhwTGKjwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fcnrQEhKl_o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e488c5e0-6084-457b-b882-39bab991b26f"
      },
      "source": [
        "parameters, loss_history = TwoLayerNN(0.05, 1500, 200, train_, tr_labels, val, val_labels) # checking with different learning rate and epoch"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 1500 :loss 6.568068769539004\n",
            "training accuracy: 0.105 and validation accuracy: 0.1077\n",
            "iteration 10 / 1500 :loss 2.7311103187149013\n",
            "training accuracy: 0.265 and validation accuracy: 0.2168\n",
            "iteration 20 / 1500 :loss 2.233616134833674\n",
            "training accuracy: 0.335 and validation accuracy: 0.331\n",
            "iteration 30 / 1500 :loss 1.7769734321398851\n",
            "training accuracy: 0.435 and validation accuracy: 0.4151\n",
            "iteration 40 / 1500 :loss 1.5507838687920623\n",
            "training accuracy: 0.51 and validation accuracy: 0.4752\n",
            "iteration 50 / 1500 :loss 1.341807903239179\n",
            "training accuracy: 0.52 and validation accuracy: 0.524\n",
            "iteration 60 / 1500 :loss 1.4581405477687286\n",
            "training accuracy: 0.56 and validation accuracy: 0.5582\n",
            "iteration 70 / 1500 :loss 1.2566192896761417\n",
            "training accuracy: 0.595 and validation accuracy: 0.5913\n",
            "iteration 80 / 1500 :loss 1.1568826702259896\n",
            "training accuracy: 0.69 and validation accuracy: 0.619\n",
            "iteration 90 / 1500 :loss 1.2016227150769376\n",
            "training accuracy: 0.585 and validation accuracy: 0.647\n",
            "iteration 100 / 1500 :loss 1.1897622950892002\n",
            "training accuracy: 0.63 and validation accuracy: 0.6697\n",
            "iteration 110 / 1500 :loss 1.0005078881959952\n",
            "training accuracy: 0.685 and validation accuracy: 0.6881\n",
            "iteration 120 / 1500 :loss 0.9052878811208597\n",
            "training accuracy: 0.725 and validation accuracy: 0.7019\n",
            "iteration 130 / 1500 :loss 0.8830800337359208\n",
            "training accuracy: 0.76 and validation accuracy: 0.7195\n",
            "iteration 140 / 1500 :loss 0.8213520404827555\n",
            "training accuracy: 0.75 and validation accuracy: 0.727\n",
            "iteration 150 / 1500 :loss 0.9553132374445893\n",
            "training accuracy: 0.69 and validation accuracy: 0.7408\n",
            "iteration 160 / 1500 :loss 0.7325463076688544\n",
            "training accuracy: 0.78 and validation accuracy: 0.7465\n",
            "iteration 170 / 1500 :loss 0.8756363751669406\n",
            "training accuracy: 0.745 and validation accuracy: 0.7573\n",
            "iteration 180 / 1500 :loss 0.8020840496566204\n",
            "training accuracy: 0.785 and validation accuracy: 0.7621\n",
            "iteration 190 / 1500 :loss 0.7447332427537193\n",
            "training accuracy: 0.815 and validation accuracy: 0.7685\n",
            "iteration 200 / 1500 :loss 0.779285882536222\n",
            "training accuracy: 0.74 and validation accuracy: 0.7767\n",
            "iteration 210 / 1500 :loss 0.7727953253415164\n",
            "training accuracy: 0.73 and validation accuracy: 0.7825\n",
            "iteration 220 / 1500 :loss 0.6924768596960075\n",
            "training accuracy: 0.785 and validation accuracy: 0.785\n",
            "iteration 230 / 1500 :loss 0.6179998491600274\n",
            "training accuracy: 0.79 and validation accuracy: 0.7921\n",
            "iteration 240 / 1500 :loss 0.6479279496508957\n",
            "training accuracy: 0.795 and validation accuracy: 0.796\n",
            "iteration 250 / 1500 :loss 0.6858250083920919\n",
            "training accuracy: 0.77 and validation accuracy: 0.8008\n",
            "iteration 260 / 1500 :loss 0.6439520307591949\n",
            "training accuracy: 0.78 and validation accuracy: 0.8044\n",
            "iteration 270 / 1500 :loss 0.6361220200724588\n",
            "training accuracy: 0.84 and validation accuracy: 0.8098\n",
            "iteration 280 / 1500 :loss 0.7020553050903972\n",
            "training accuracy: 0.815 and validation accuracy: 0.8092\n",
            "iteration 290 / 1500 :loss 0.7568917168028428\n",
            "training accuracy: 0.795 and validation accuracy: 0.814\n",
            "iteration 300 / 1500 :loss 0.7655723549486008\n",
            "training accuracy: 0.745 and validation accuracy: 0.8145\n",
            "iteration 310 / 1500 :loss 0.7350411673011653\n",
            "training accuracy: 0.815 and validation accuracy: 0.8214\n",
            "iteration 320 / 1500 :loss 0.6175260669723146\n",
            "training accuracy: 0.825 and validation accuracy: 0.8235\n",
            "iteration 330 / 1500 :loss 0.6453015999206922\n",
            "training accuracy: 0.805 and validation accuracy: 0.8245\n",
            "iteration 340 / 1500 :loss 0.6129896506358509\n",
            "training accuracy: 0.81 and validation accuracy: 0.8297\n",
            "iteration 350 / 1500 :loss 0.6806503752011682\n",
            "training accuracy: 0.76 and validation accuracy: 0.8322\n",
            "iteration 360 / 1500 :loss 0.580993543138501\n",
            "training accuracy: 0.8 and validation accuracy: 0.8339\n",
            "iteration 370 / 1500 :loss 0.41935694567876636\n",
            "training accuracy: 0.865 and validation accuracy: 0.8377\n",
            "iteration 380 / 1500 :loss 0.46906140586286527\n",
            "training accuracy: 0.86 and validation accuracy: 0.8361\n",
            "iteration 390 / 1500 :loss 0.5350370087223847\n",
            "training accuracy: 0.845 and validation accuracy: 0.8384\n",
            "iteration 400 / 1500 :loss 0.38112828335457893\n",
            "training accuracy: 0.93 and validation accuracy: 0.8408\n",
            "iteration 410 / 1500 :loss 0.6016308612992743\n",
            "training accuracy: 0.82 and validation accuracy: 0.8394\n",
            "iteration 420 / 1500 :loss 0.5878650861410231\n",
            "training accuracy: 0.835 and validation accuracy: 0.843\n",
            "iteration 430 / 1500 :loss 0.6897293020400282\n",
            "training accuracy: 0.8 and validation accuracy: 0.8469\n",
            "iteration 440 / 1500 :loss 0.5483254428151603\n",
            "training accuracy: 0.835 and validation accuracy: 0.8472\n",
            "iteration 450 / 1500 :loss 0.46195737388492253\n",
            "training accuracy: 0.835 and validation accuracy: 0.847\n",
            "iteration 460 / 1500 :loss 0.45890709187999634\n",
            "training accuracy: 0.875 and validation accuracy: 0.8499\n",
            "iteration 470 / 1500 :loss 0.43880362849362425\n",
            "training accuracy: 0.835 and validation accuracy: 0.8519\n",
            "iteration 480 / 1500 :loss 0.5291462702112494\n",
            "training accuracy: 0.85 and validation accuracy: 0.8529\n",
            "iteration 490 / 1500 :loss 0.49878319440679747\n",
            "training accuracy: 0.87 and validation accuracy: 0.8535\n",
            "iteration 500 / 1500 :loss 0.5911459555606957\n",
            "training accuracy: 0.815 and validation accuracy: 0.8553\n",
            "iteration 510 / 1500 :loss 0.5805823609594473\n",
            "training accuracy: 0.845 and validation accuracy: 0.8547\n",
            "iteration 520 / 1500 :loss 0.5449178287375683\n",
            "training accuracy: 0.85 and validation accuracy: 0.8564\n",
            "iteration 530 / 1500 :loss 0.5186712529926746\n",
            "training accuracy: 0.87 and validation accuracy: 0.858\n",
            "iteration 540 / 1500 :loss 0.618508496987674\n",
            "training accuracy: 0.82 and validation accuracy: 0.861\n",
            "iteration 550 / 1500 :loss 0.569214318809751\n",
            "training accuracy: 0.865 and validation accuracy: 0.8605\n",
            "iteration 560 / 1500 :loss 0.367326445993954\n",
            "training accuracy: 0.895 and validation accuracy: 0.8622\n",
            "iteration 570 / 1500 :loss 0.4280744903505771\n",
            "training accuracy: 0.875 and validation accuracy: 0.8645\n",
            "iteration 580 / 1500 :loss 0.49456360961245294\n",
            "training accuracy: 0.865 and validation accuracy: 0.8645\n",
            "iteration 590 / 1500 :loss 0.5188801045295435\n",
            "training accuracy: 0.875 and validation accuracy: 0.8646\n",
            "iteration 600 / 1500 :loss 0.5173980454416373\n",
            "training accuracy: 0.835 and validation accuracy: 0.865\n",
            "iteration 610 / 1500 :loss 0.4699529233749547\n",
            "training accuracy: 0.855 and validation accuracy: 0.8672\n",
            "iteration 620 / 1500 :loss 0.4482019805776465\n",
            "training accuracy: 0.89 and validation accuracy: 0.8676\n",
            "iteration 630 / 1500 :loss 0.4991614917046016\n",
            "training accuracy: 0.845 and validation accuracy: 0.868\n",
            "iteration 640 / 1500 :loss 0.41441591668969346\n",
            "training accuracy: 0.89 and validation accuracy: 0.8674\n",
            "iteration 650 / 1500 :loss 0.4463594466818908\n",
            "training accuracy: 0.875 and validation accuracy: 0.8684\n",
            "iteration 660 / 1500 :loss 0.37693887178339147\n",
            "training accuracy: 0.885 and validation accuracy: 0.8695\n",
            "iteration 670 / 1500 :loss 0.47935200821712515\n",
            "training accuracy: 0.83 and validation accuracy: 0.8698\n",
            "iteration 680 / 1500 :loss 0.3188884949329506\n",
            "training accuracy: 0.905 and validation accuracy: 0.8691\n",
            "iteration 690 / 1500 :loss 0.4500960680050409\n",
            "training accuracy: 0.845 and validation accuracy: 0.8716\n",
            "iteration 700 / 1500 :loss 0.5711358518235088\n",
            "training accuracy: 0.845 and validation accuracy: 0.8736\n",
            "iteration 710 / 1500 :loss 0.49387856020072235\n",
            "training accuracy: 0.875 and validation accuracy: 0.8732\n",
            "iteration 720 / 1500 :loss 0.428061624642532\n",
            "training accuracy: 0.875 and validation accuracy: 0.8742\n",
            "iteration 730 / 1500 :loss 0.45368265775350436\n",
            "training accuracy: 0.88 and validation accuracy: 0.8743\n",
            "iteration 740 / 1500 :loss 0.5171504674116961\n",
            "training accuracy: 0.86 and validation accuracy: 0.875\n",
            "iteration 750 / 1500 :loss 0.511210506694551\n",
            "training accuracy: 0.84 and validation accuracy: 0.8761\n",
            "iteration 760 / 1500 :loss 0.42085849153103255\n",
            "training accuracy: 0.875 and validation accuracy: 0.8727\n",
            "iteration 770 / 1500 :loss 0.43009535154100276\n",
            "training accuracy: 0.89 and validation accuracy: 0.8764\n",
            "iteration 780 / 1500 :loss 0.3147463285362714\n",
            "training accuracy: 0.91 and validation accuracy: 0.8775\n",
            "iteration 790 / 1500 :loss 0.38679674281775844\n",
            "training accuracy: 0.895 and validation accuracy: 0.8768\n",
            "iteration 800 / 1500 :loss 0.42977578515467046\n",
            "training accuracy: 0.88 and validation accuracy: 0.879\n",
            "iteration 810 / 1500 :loss 0.49294525958800084\n",
            "training accuracy: 0.855 and validation accuracy: 0.8794\n",
            "iteration 820 / 1500 :loss 0.39219000089715705\n",
            "training accuracy: 0.91 and validation accuracy: 0.8764\n",
            "iteration 830 / 1500 :loss 0.48531416786273013\n",
            "training accuracy: 0.865 and validation accuracy: 0.8798\n",
            "iteration 840 / 1500 :loss 0.3914538072523188\n",
            "training accuracy: 0.88 and validation accuracy: 0.8785\n",
            "iteration 850 / 1500 :loss 0.33075006613735325\n",
            "training accuracy: 0.905 and validation accuracy: 0.8793\n",
            "iteration 860 / 1500 :loss 0.3971684175044998\n",
            "training accuracy: 0.875 and validation accuracy: 0.8802\n",
            "iteration 870 / 1500 :loss 0.5338427382091424\n",
            "training accuracy: 0.86 and validation accuracy: 0.8814\n",
            "iteration 880 / 1500 :loss 0.327017853297357\n",
            "training accuracy: 0.91 and validation accuracy: 0.8805\n",
            "iteration 890 / 1500 :loss 0.418592942950196\n",
            "training accuracy: 0.88 and validation accuracy: 0.8826\n",
            "iteration 900 / 1500 :loss 0.47307898166359375\n",
            "training accuracy: 0.85 and validation accuracy: 0.8815\n",
            "iteration 910 / 1500 :loss 0.3748735481741257\n",
            "training accuracy: 0.87 and validation accuracy: 0.8834\n",
            "iteration 920 / 1500 :loss 0.6098698972265842\n",
            "training accuracy: 0.83 and validation accuracy: 0.884\n",
            "iteration 930 / 1500 :loss 0.3448577342577556\n",
            "training accuracy: 0.915 and validation accuracy: 0.8863\n",
            "iteration 940 / 1500 :loss 0.5120202803055075\n",
            "training accuracy: 0.865 and validation accuracy: 0.8858\n",
            "iteration 950 / 1500 :loss 0.288464963812632\n",
            "training accuracy: 0.905 and validation accuracy: 0.8861\n",
            "iteration 960 / 1500 :loss 0.44360072544652907\n",
            "training accuracy: 0.88 and validation accuracy: 0.8838\n",
            "iteration 970 / 1500 :loss 0.4321234194310915\n",
            "training accuracy: 0.87 and validation accuracy: 0.8862\n",
            "iteration 980 / 1500 :loss 0.41276006738235466\n",
            "training accuracy: 0.865 and validation accuracy: 0.8858\n",
            "iteration 990 / 1500 :loss 0.461549945471463\n",
            "training accuracy: 0.88 and validation accuracy: 0.8869\n",
            "iteration 1000 / 1500 :loss 0.4445137745680195\n",
            "training accuracy: 0.89 and validation accuracy: 0.8861\n",
            "iteration 1010 / 1500 :loss 0.4394275328793671\n",
            "training accuracy: 0.89 and validation accuracy: 0.8874\n",
            "iteration 1020 / 1500 :loss 0.4233951053647309\n",
            "training accuracy: 0.89 and validation accuracy: 0.8876\n",
            "iteration 1030 / 1500 :loss 0.5245821772464894\n",
            "training accuracy: 0.82 and validation accuracy: 0.8885\n",
            "iteration 1040 / 1500 :loss 0.4531765414699983\n",
            "training accuracy: 0.88 and validation accuracy: 0.8892\n",
            "iteration 1050 / 1500 :loss 0.3924300137364775\n",
            "training accuracy: 0.89 and validation accuracy: 0.8889\n",
            "iteration 1060 / 1500 :loss 0.4339969631051563\n",
            "training accuracy: 0.855 and validation accuracy: 0.8879\n",
            "iteration 1070 / 1500 :loss 0.4447617065434993\n",
            "training accuracy: 0.855 and validation accuracy: 0.8886\n",
            "iteration 1080 / 1500 :loss 0.35080460087779675\n",
            "training accuracy: 0.89 and validation accuracy: 0.8882\n",
            "iteration 1090 / 1500 :loss 0.5561023676553166\n",
            "training accuracy: 0.845 and validation accuracy: 0.8869\n",
            "iteration 1100 / 1500 :loss 0.3524848702558617\n",
            "training accuracy: 0.885 and validation accuracy: 0.8892\n",
            "iteration 1110 / 1500 :loss 0.4100696377406367\n",
            "training accuracy: 0.875 and validation accuracy: 0.8889\n",
            "iteration 1120 / 1500 :loss 0.31412344428619804\n",
            "training accuracy: 0.915 and validation accuracy: 0.8897\n",
            "iteration 1130 / 1500 :loss 0.35311996705209786\n",
            "training accuracy: 0.885 and validation accuracy: 0.8887\n",
            "iteration 1140 / 1500 :loss 0.34067591059706986\n",
            "training accuracy: 0.91 and validation accuracy: 0.8924\n",
            "iteration 1150 / 1500 :loss 0.39885326867399507\n",
            "training accuracy: 0.89 and validation accuracy: 0.891\n",
            "iteration 1160 / 1500 :loss 0.3491151742881526\n",
            "training accuracy: 0.915 and validation accuracy: 0.8891\n",
            "iteration 1170 / 1500 :loss 0.3341130921571924\n",
            "training accuracy: 0.91 and validation accuracy: 0.8928\n",
            "iteration 1180 / 1500 :loss 0.4397159652635858\n",
            "training accuracy: 0.895 and validation accuracy: 0.8917\n",
            "iteration 1190 / 1500 :loss 0.358300857911222\n",
            "training accuracy: 0.885 and validation accuracy: 0.891\n",
            "iteration 1200 / 1500 :loss 0.43209928186687885\n",
            "training accuracy: 0.89 and validation accuracy: 0.8932\n",
            "iteration 1210 / 1500 :loss 0.33991507593942627\n",
            "training accuracy: 0.93 and validation accuracy: 0.8934\n",
            "iteration 1220 / 1500 :loss 0.5009542307434693\n",
            "training accuracy: 0.855 and validation accuracy: 0.8936\n",
            "iteration 1230 / 1500 :loss 0.4137333018091317\n",
            "training accuracy: 0.885 and validation accuracy: 0.8927\n",
            "iteration 1240 / 1500 :loss 0.28568634969490864\n",
            "training accuracy: 0.905 and validation accuracy: 0.8941\n",
            "iteration 1250 / 1500 :loss 0.3414085294865913\n",
            "training accuracy: 0.895 and validation accuracy: 0.8935\n",
            "iteration 1260 / 1500 :loss 0.3961494853576177\n",
            "training accuracy: 0.91 and validation accuracy: 0.8945\n",
            "iteration 1270 / 1500 :loss 0.3990235488982299\n",
            "training accuracy: 0.855 and validation accuracy: 0.8923\n",
            "iteration 1280 / 1500 :loss 0.40189795919217686\n",
            "training accuracy: 0.895 and validation accuracy: 0.8942\n",
            "iteration 1290 / 1500 :loss 0.3587545068020463\n",
            "training accuracy: 0.905 and validation accuracy: 0.895\n",
            "iteration 1300 / 1500 :loss 0.3705179474167283\n",
            "training accuracy: 0.92 and validation accuracy: 0.8953\n",
            "iteration 1310 / 1500 :loss 0.3504540189886619\n",
            "training accuracy: 0.88 and validation accuracy: 0.8974\n",
            "iteration 1320 / 1500 :loss 0.4009259309891142\n",
            "training accuracy: 0.89 and validation accuracy: 0.8964\n",
            "iteration 1330 / 1500 :loss 0.32887607709412625\n",
            "training accuracy: 0.91 and validation accuracy: 0.8962\n",
            "iteration 1340 / 1500 :loss 0.43647754462232496\n",
            "training accuracy: 0.87 and validation accuracy: 0.8946\n",
            "iteration 1350 / 1500 :loss 0.3191719332982249\n",
            "training accuracy: 0.905 and validation accuracy: 0.8953\n",
            "iteration 1360 / 1500 :loss 0.35998590836649436\n",
            "training accuracy: 0.88 and validation accuracy: 0.8966\n",
            "iteration 1370 / 1500 :loss 0.29389142490982595\n",
            "training accuracy: 0.905 and validation accuracy: 0.8975\n",
            "iteration 1380 / 1500 :loss 0.2774860216262548\n",
            "training accuracy: 0.885 and validation accuracy: 0.8973\n",
            "iteration 1390 / 1500 :loss 0.37724722087715873\n",
            "training accuracy: 0.92 and validation accuracy: 0.897\n",
            "iteration 1400 / 1500 :loss 0.3325384139594628\n",
            "training accuracy: 0.91 and validation accuracy: 0.8981\n",
            "iteration 1410 / 1500 :loss 0.3505678074956438\n",
            "training accuracy: 0.895 and validation accuracy: 0.8994\n",
            "iteration 1420 / 1500 :loss 0.37290753334683285\n",
            "training accuracy: 0.915 and validation accuracy: 0.8978\n",
            "iteration 1430 / 1500 :loss 0.29072652624980466\n",
            "training accuracy: 0.915 and validation accuracy: 0.8973\n",
            "iteration 1440 / 1500 :loss 0.3596946111384686\n",
            "training accuracy: 0.88 and validation accuracy: 0.8981\n",
            "iteration 1450 / 1500 :loss 0.2867869469394942\n",
            "training accuracy: 0.9 and validation accuracy: 0.8988\n",
            "iteration 1460 / 1500 :loss 0.41803699676644085\n",
            "training accuracy: 0.88 and validation accuracy: 0.8985\n",
            "iteration 1470 / 1500 :loss 0.30663919095463155\n",
            "training accuracy: 0.905 and validation accuracy: 0.899\n",
            "iteration 1480 / 1500 :loss 0.29863352990230185\n",
            "training accuracy: 0.9 and validation accuracy: 0.9002\n",
            "iteration 1490 / 1500 :loss 0.3009417874937192\n",
            "training accuracy: 0.915 and validation accuracy: 0.8996\n",
            "iteration 1500 / 1500 :loss 0.3560471755210844\n",
            "training accuracy: 0.885 and validation accuracy: 0.9003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kae1oRXiNBtU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "405a5381-ab69-447d-a951-6d7fcad71358"
      },
      "source": [
        "parameters, loss_history = TwoLayerNN(0.1, 2000, 200, train_, tr_labels, val, val_labels)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 2000 :loss 4.378151976933361\n",
            "training accuracy: 0.205 and validation accuracy: 0.167\n",
            "iteration 10 / 2000 :loss 2.4020162008535118\n",
            "training accuracy: 0.295 and validation accuracy: 0.3419\n",
            "iteration 20 / 2000 :loss 1.3807227551048278\n",
            "training accuracy: 0.555 and validation accuracy: 0.4794\n",
            "iteration 30 / 2000 :loss 1.3742549085193607\n",
            "training accuracy: 0.555 and validation accuracy: 0.58\n",
            "iteration 40 / 2000 :loss 1.1988975822159404\n",
            "training accuracy: 0.625 and validation accuracy: 0.6436\n",
            "iteration 50 / 2000 :loss 1.0598071356373024\n",
            "training accuracy: 0.7 and validation accuracy: 0.6861\n",
            "iteration 60 / 2000 :loss 0.8882535357373266\n",
            "training accuracy: 0.755 and validation accuracy: 0.7182\n",
            "iteration 70 / 2000 :loss 0.7585899558749546\n",
            "training accuracy: 0.765 and validation accuracy: 0.7434\n",
            "iteration 80 / 2000 :loss 0.8214239844573151\n",
            "training accuracy: 0.72 and validation accuracy: 0.7631\n",
            "iteration 90 / 2000 :loss 0.6458534156403879\n",
            "training accuracy: 0.825 and validation accuracy: 0.7745\n",
            "iteration 100 / 2000 :loss 0.7139776646848506\n",
            "training accuracy: 0.795 and validation accuracy: 0.7871\n",
            "iteration 110 / 2000 :loss 0.6910513555015761\n",
            "training accuracy: 0.775 and validation accuracy: 0.794\n",
            "iteration 120 / 2000 :loss 0.6719082965636558\n",
            "training accuracy: 0.82 and validation accuracy: 0.8042\n",
            "iteration 130 / 2000 :loss 0.7624676039246654\n",
            "training accuracy: 0.75 and validation accuracy: 0.8098\n",
            "iteration 140 / 2000 :loss 0.5911583753258978\n",
            "training accuracy: 0.8 and validation accuracy: 0.8163\n",
            "iteration 150 / 2000 :loss 0.5370994088879876\n",
            "training accuracy: 0.86 and validation accuracy: 0.8216\n",
            "iteration 160 / 2000 :loss 0.5389524792215284\n",
            "training accuracy: 0.815 and validation accuracy: 0.8242\n",
            "iteration 170 / 2000 :loss 0.5671574448857839\n",
            "training accuracy: 0.825 and validation accuracy: 0.8272\n",
            "iteration 180 / 2000 :loss 0.5770206816804409\n",
            "training accuracy: 0.845 and validation accuracy: 0.8326\n",
            "iteration 190 / 2000 :loss 0.5895216455574152\n",
            "training accuracy: 0.85 and validation accuracy: 0.8407\n",
            "iteration 200 / 2000 :loss 0.4205506769573269\n",
            "training accuracy: 0.865 and validation accuracy: 0.8463\n",
            "iteration 210 / 2000 :loss 0.6096719869421806\n",
            "training accuracy: 0.79 and validation accuracy: 0.846\n",
            "iteration 220 / 2000 :loss 0.5641963898003939\n",
            "training accuracy: 0.81 and validation accuracy: 0.8472\n",
            "iteration 230 / 2000 :loss 0.5010623438169519\n",
            "training accuracy: 0.855 and validation accuracy: 0.8522\n",
            "iteration 240 / 2000 :loss 0.3527845729691515\n",
            "training accuracy: 0.88 and validation accuracy: 0.8515\n",
            "iteration 250 / 2000 :loss 0.5153600697555893\n",
            "training accuracy: 0.845 and validation accuracy: 0.8565\n",
            "iteration 260 / 2000 :loss 0.5350192947776804\n",
            "training accuracy: 0.84 and validation accuracy: 0.8544\n",
            "iteration 270 / 2000 :loss 0.529627974550058\n",
            "training accuracy: 0.855 and validation accuracy: 0.8624\n",
            "iteration 280 / 2000 :loss 0.45015659395209495\n",
            "training accuracy: 0.855 and validation accuracy: 0.864\n",
            "iteration 290 / 2000 :loss 0.5540653150421294\n",
            "training accuracy: 0.845 and validation accuracy: 0.8651\n",
            "iteration 300 / 2000 :loss 0.4635729028868697\n",
            "training accuracy: 0.855 and validation accuracy: 0.8648\n",
            "iteration 310 / 2000 :loss 0.4545284756373451\n",
            "training accuracy: 0.865 and validation accuracy: 0.8671\n",
            "iteration 320 / 2000 :loss 0.47295056020820847\n",
            "training accuracy: 0.865 and validation accuracy: 0.8703\n",
            "iteration 330 / 2000 :loss 0.4998688514908254\n",
            "training accuracy: 0.89 and validation accuracy: 0.8698\n",
            "iteration 340 / 2000 :loss 0.5199549820615755\n",
            "training accuracy: 0.85 and validation accuracy: 0.8731\n",
            "iteration 350 / 2000 :loss 0.49911552276925536\n",
            "training accuracy: 0.875 and validation accuracy: 0.876\n",
            "iteration 360 / 2000 :loss 0.369150774768404\n",
            "training accuracy: 0.9 and validation accuracy: 0.8752\n",
            "iteration 370 / 2000 :loss 0.5983898570455589\n",
            "training accuracy: 0.84 and validation accuracy: 0.876\n",
            "iteration 380 / 2000 :loss 0.5177210945873985\n",
            "training accuracy: 0.86 and validation accuracy: 0.8759\n",
            "iteration 390 / 2000 :loss 0.454581246033231\n",
            "training accuracy: 0.84 and validation accuracy: 0.8767\n",
            "iteration 400 / 2000 :loss 0.3334767635655132\n",
            "training accuracy: 0.89 and validation accuracy: 0.8792\n",
            "iteration 410 / 2000 :loss 0.4139625997449481\n",
            "training accuracy: 0.895 and validation accuracy: 0.8813\n",
            "iteration 420 / 2000 :loss 0.41348822864478707\n",
            "training accuracy: 0.9 and validation accuracy: 0.8794\n",
            "iteration 430 / 2000 :loss 0.42684890589666513\n",
            "training accuracy: 0.885 and validation accuracy: 0.8824\n",
            "iteration 440 / 2000 :loss 0.427208699666438\n",
            "training accuracy: 0.85 and validation accuracy: 0.8828\n",
            "iteration 450 / 2000 :loss 0.4322706356550494\n",
            "training accuracy: 0.86 and validation accuracy: 0.8826\n",
            "iteration 460 / 2000 :loss 0.49290847324303383\n",
            "training accuracy: 0.875 and validation accuracy: 0.8824\n",
            "iteration 470 / 2000 :loss 0.37459617996459554\n",
            "training accuracy: 0.865 and validation accuracy: 0.8872\n",
            "iteration 480 / 2000 :loss 0.5891885951124604\n",
            "training accuracy: 0.855 and validation accuracy: 0.8856\n",
            "iteration 490 / 2000 :loss 0.4035991671028876\n",
            "training accuracy: 0.89 and validation accuracy: 0.8868\n",
            "iteration 500 / 2000 :loss 0.42948806448819665\n",
            "training accuracy: 0.89 and validation accuracy: 0.8872\n",
            "iteration 510 / 2000 :loss 0.30259434527086265\n",
            "training accuracy: 0.925 and validation accuracy: 0.8903\n",
            "iteration 520 / 2000 :loss 0.33735371567418854\n",
            "training accuracy: 0.925 and validation accuracy: 0.8896\n",
            "iteration 530 / 2000 :loss 0.3336885999695771\n",
            "training accuracy: 0.915 and validation accuracy: 0.8916\n",
            "iteration 540 / 2000 :loss 0.36706025683080384\n",
            "training accuracy: 0.895 and validation accuracy: 0.8898\n",
            "iteration 550 / 2000 :loss 0.39701071984908126\n",
            "training accuracy: 0.89 and validation accuracy: 0.8922\n",
            "iteration 560 / 2000 :loss 0.5168931577786625\n",
            "training accuracy: 0.84 and validation accuracy: 0.8919\n",
            "iteration 570 / 2000 :loss 0.335850383346153\n",
            "training accuracy: 0.9 and validation accuracy: 0.8941\n",
            "iteration 580 / 2000 :loss 0.41883361448660594\n",
            "training accuracy: 0.885 and validation accuracy: 0.8937\n",
            "iteration 590 / 2000 :loss 0.43740241580754763\n",
            "training accuracy: 0.89 and validation accuracy: 0.8967\n",
            "iteration 600 / 2000 :loss 0.2944709270880608\n",
            "training accuracy: 0.94 and validation accuracy: 0.8951\n",
            "iteration 610 / 2000 :loss 0.5106068378551712\n",
            "training accuracy: 0.855 and validation accuracy: 0.8922\n",
            "iteration 620 / 2000 :loss 0.4561268195579488\n",
            "training accuracy: 0.88 and validation accuracy: 0.8954\n",
            "iteration 630 / 2000 :loss 0.356786786424664\n",
            "training accuracy: 0.905 and validation accuracy: 0.8965\n",
            "iteration 640 / 2000 :loss 0.4305369822717043\n",
            "training accuracy: 0.91 and validation accuracy: 0.895\n",
            "iteration 650 / 2000 :loss 0.2882370468290398\n",
            "training accuracy: 0.91 and validation accuracy: 0.8934\n",
            "iteration 660 / 2000 :loss 0.39215593112964114\n",
            "training accuracy: 0.895 and validation accuracy: 0.8976\n",
            "iteration 670 / 2000 :loss 0.3000692845064313\n",
            "training accuracy: 0.92 and validation accuracy: 0.8986\n",
            "iteration 680 / 2000 :loss 0.31241067666759187\n",
            "training accuracy: 0.9 and validation accuracy: 0.8985\n",
            "iteration 690 / 2000 :loss 0.3816863651224325\n",
            "training accuracy: 0.91 and validation accuracy: 0.8991\n",
            "iteration 700 / 2000 :loss 0.3870441807601358\n",
            "training accuracy: 0.92 and validation accuracy: 0.9005\n",
            "iteration 710 / 2000 :loss 0.35190175526749934\n",
            "training accuracy: 0.89 and validation accuracy: 0.8996\n",
            "iteration 720 / 2000 :loss 0.4275170478569001\n",
            "training accuracy: 0.885 and validation accuracy: 0.8992\n",
            "iteration 730 / 2000 :loss 0.3774963134255286\n",
            "training accuracy: 0.88 and validation accuracy: 0.9028\n",
            "iteration 740 / 2000 :loss 0.43998770599184256\n",
            "training accuracy: 0.865 and validation accuracy: 0.9031\n",
            "iteration 750 / 2000 :loss 0.3768338891057158\n",
            "training accuracy: 0.895 and validation accuracy: 0.9035\n",
            "iteration 760 / 2000 :loss 0.39758690683211434\n",
            "training accuracy: 0.905 and validation accuracy: 0.9016\n",
            "iteration 770 / 2000 :loss 0.24057339497532204\n",
            "training accuracy: 0.935 and validation accuracy: 0.9027\n",
            "iteration 780 / 2000 :loss 0.3492420098896285\n",
            "training accuracy: 0.895 and validation accuracy: 0.9015\n",
            "iteration 790 / 2000 :loss 0.34936663534502266\n",
            "training accuracy: 0.9 and validation accuracy: 0.8985\n",
            "iteration 800 / 2000 :loss 0.369956385723428\n",
            "training accuracy: 0.885 and validation accuracy: 0.9042\n",
            "iteration 810 / 2000 :loss 0.3241261849544369\n",
            "training accuracy: 0.915 and validation accuracy: 0.9049\n",
            "iteration 820 / 2000 :loss 0.22307768529210736\n",
            "training accuracy: 0.955 and validation accuracy: 0.9047\n",
            "iteration 830 / 2000 :loss 0.36796944209526317\n",
            "training accuracy: 0.91 and validation accuracy: 0.9056\n",
            "iteration 840 / 2000 :loss 0.39236891071664254\n",
            "training accuracy: 0.88 and validation accuracy: 0.9044\n",
            "iteration 850 / 2000 :loss 0.30197638710350844\n",
            "training accuracy: 0.92 and validation accuracy: 0.906\n",
            "iteration 860 / 2000 :loss 0.30325297176188565\n",
            "training accuracy: 0.91 and validation accuracy: 0.9077\n",
            "iteration 870 / 2000 :loss 0.3204015977660686\n",
            "training accuracy: 0.91 and validation accuracy: 0.9092\n",
            "iteration 880 / 2000 :loss 0.3864659183508483\n",
            "training accuracy: 0.89 and validation accuracy: 0.9077\n",
            "iteration 890 / 2000 :loss 0.33384834605673197\n",
            "training accuracy: 0.925 and validation accuracy: 0.909\n",
            "iteration 900 / 2000 :loss 0.3408766236155833\n",
            "training accuracy: 0.885 and validation accuracy: 0.9089\n",
            "iteration 910 / 2000 :loss 0.39026154306927596\n",
            "training accuracy: 0.935 and validation accuracy: 0.9057\n",
            "iteration 920 / 2000 :loss 0.23751567151785313\n",
            "training accuracy: 0.945 and validation accuracy: 0.909\n",
            "iteration 930 / 2000 :loss 0.3598148349100733\n",
            "training accuracy: 0.915 and validation accuracy: 0.9091\n",
            "iteration 940 / 2000 :loss 0.19921038060863677\n",
            "training accuracy: 0.95 and validation accuracy: 0.906\n",
            "iteration 950 / 2000 :loss 0.37951462220996823\n",
            "training accuracy: 0.89 and validation accuracy: 0.9084\n",
            "iteration 960 / 2000 :loss 0.34790434039665696\n",
            "training accuracy: 0.89 and validation accuracy: 0.9096\n",
            "iteration 970 / 2000 :loss 0.2457724350225066\n",
            "training accuracy: 0.935 and validation accuracy: 0.9104\n",
            "iteration 980 / 2000 :loss 0.3228655146973729\n",
            "training accuracy: 0.915 and validation accuracy: 0.9092\n",
            "iteration 990 / 2000 :loss 0.35022394433152415\n",
            "training accuracy: 0.925 and validation accuracy: 0.9073\n",
            "iteration 1000 / 2000 :loss 0.24293390101706958\n",
            "training accuracy: 0.935 and validation accuracy: 0.9085\n",
            "iteration 1010 / 2000 :loss 0.2970284146864575\n",
            "training accuracy: 0.89 and validation accuracy: 0.9087\n",
            "iteration 1020 / 2000 :loss 0.3264601262303449\n",
            "training accuracy: 0.935 and validation accuracy: 0.9121\n",
            "iteration 1030 / 2000 :loss 0.3864680169789426\n",
            "training accuracy: 0.905 and validation accuracy: 0.9123\n",
            "iteration 1040 / 2000 :loss 0.39467226078802314\n",
            "training accuracy: 0.875 and validation accuracy: 0.9112\n",
            "iteration 1050 / 2000 :loss 0.33909270654268026\n",
            "training accuracy: 0.905 and validation accuracy: 0.911\n",
            "iteration 1060 / 2000 :loss 0.27411525550877786\n",
            "training accuracy: 0.925 and validation accuracy: 0.9109\n",
            "iteration 1070 / 2000 :loss 0.4248170802461638\n",
            "training accuracy: 0.865 and validation accuracy: 0.9113\n",
            "iteration 1080 / 2000 :loss 0.30320742289876446\n",
            "training accuracy: 0.925 and validation accuracy: 0.9132\n",
            "iteration 1090 / 2000 :loss 0.24879002319386934\n",
            "training accuracy: 0.925 and validation accuracy: 0.9133\n",
            "iteration 1100 / 2000 :loss 0.21170646468268287\n",
            "training accuracy: 0.935 and validation accuracy: 0.9136\n",
            "iteration 1110 / 2000 :loss 0.3206331042381383\n",
            "training accuracy: 0.9 and validation accuracy: 0.9148\n",
            "iteration 1120 / 2000 :loss 0.3974565993027296\n",
            "training accuracy: 0.885 and validation accuracy: 0.9142\n",
            "iteration 1130 / 2000 :loss 0.20240396114214795\n",
            "training accuracy: 0.935 and validation accuracy: 0.9139\n",
            "iteration 1140 / 2000 :loss 0.2609577396740795\n",
            "training accuracy: 0.94 and validation accuracy: 0.9151\n",
            "iteration 1150 / 2000 :loss 0.28676219659477814\n",
            "training accuracy: 0.925 and validation accuracy: 0.9162\n",
            "iteration 1160 / 2000 :loss 0.3577137466276964\n",
            "training accuracy: 0.895 and validation accuracy: 0.9121\n",
            "iteration 1170 / 2000 :loss 0.3330827779363352\n",
            "training accuracy: 0.92 and validation accuracy: 0.9164\n",
            "iteration 1180 / 2000 :loss 0.2667846821807236\n",
            "training accuracy: 0.925 and validation accuracy: 0.917\n",
            "iteration 1190 / 2000 :loss 0.36310398615038364\n",
            "training accuracy: 0.93 and validation accuracy: 0.916\n",
            "iteration 1200 / 2000 :loss 0.44790199638499006\n",
            "training accuracy: 0.88 and validation accuracy: 0.9154\n",
            "iteration 1210 / 2000 :loss 0.22153233914993847\n",
            "training accuracy: 0.94 and validation accuracy: 0.9161\n",
            "iteration 1220 / 2000 :loss 0.29487896010768344\n",
            "training accuracy: 0.92 and validation accuracy: 0.9172\n",
            "iteration 1230 / 2000 :loss 0.2361513383877259\n",
            "training accuracy: 0.94 and validation accuracy: 0.916\n",
            "iteration 1240 / 2000 :loss 0.28932033763394505\n",
            "training accuracy: 0.925 and validation accuracy: 0.9171\n",
            "iteration 1250 / 2000 :loss 0.286393914505806\n",
            "training accuracy: 0.91 and validation accuracy: 0.9152\n",
            "iteration 1260 / 2000 :loss 0.3746898040060534\n",
            "training accuracy: 0.885 and validation accuracy: 0.9176\n",
            "iteration 1270 / 2000 :loss 0.277101939081722\n",
            "training accuracy: 0.925 and validation accuracy: 0.9182\n",
            "iteration 1280 / 2000 :loss 0.2853392088775918\n",
            "training accuracy: 0.925 and validation accuracy: 0.9166\n",
            "iteration 1290 / 2000 :loss 0.24955933651232037\n",
            "training accuracy: 0.935 and validation accuracy: 0.9188\n",
            "iteration 1300 / 2000 :loss 0.3389373584180512\n",
            "training accuracy: 0.9 and validation accuracy: 0.9179\n",
            "iteration 1310 / 2000 :loss 0.23896454042340826\n",
            "training accuracy: 0.925 and validation accuracy: 0.9174\n",
            "iteration 1320 / 2000 :loss 0.34235636049949014\n",
            "training accuracy: 0.92 and validation accuracy: 0.9182\n",
            "iteration 1330 / 2000 :loss 0.3020499305021042\n",
            "training accuracy: 0.93 and validation accuracy: 0.9192\n",
            "iteration 1340 / 2000 :loss 0.2909913505054299\n",
            "training accuracy: 0.93 and validation accuracy: 0.9182\n",
            "iteration 1350 / 2000 :loss 0.2937702518570332\n",
            "training accuracy: 0.925 and validation accuracy: 0.9191\n",
            "iteration 1360 / 2000 :loss 0.24153747297408276\n",
            "training accuracy: 0.925 and validation accuracy: 0.9199\n",
            "iteration 1370 / 2000 :loss 0.2698476793421492\n",
            "training accuracy: 0.945 and validation accuracy: 0.9202\n",
            "iteration 1380 / 2000 :loss 0.22387411784034278\n",
            "training accuracy: 0.955 and validation accuracy: 0.9196\n",
            "iteration 1390 / 2000 :loss 0.21576664212799326\n",
            "training accuracy: 0.92 and validation accuracy: 0.9187\n",
            "iteration 1400 / 2000 :loss 0.2359559183240364\n",
            "training accuracy: 0.925 and validation accuracy: 0.9194\n",
            "iteration 1410 / 2000 :loss 0.2528689514191371\n",
            "training accuracy: 0.93 and validation accuracy: 0.921\n",
            "iteration 1420 / 2000 :loss 0.26958225432766403\n",
            "training accuracy: 0.93 and validation accuracy: 0.9209\n",
            "iteration 1430 / 2000 :loss 0.37119917572374206\n",
            "training accuracy: 0.93 and validation accuracy: 0.9189\n",
            "iteration 1440 / 2000 :loss 0.24778520823666678\n",
            "training accuracy: 0.92 and validation accuracy: 0.9182\n",
            "iteration 1450 / 2000 :loss 0.29362959248530607\n",
            "training accuracy: 0.915 and validation accuracy: 0.9209\n",
            "iteration 1460 / 2000 :loss 0.3887084454064395\n",
            "training accuracy: 0.92 and validation accuracy: 0.9175\n",
            "iteration 1470 / 2000 :loss 0.3725680623852071\n",
            "training accuracy: 0.905 and validation accuracy: 0.9212\n",
            "iteration 1480 / 2000 :loss 0.3011538202855492\n",
            "training accuracy: 0.915 and validation accuracy: 0.9188\n",
            "iteration 1490 / 2000 :loss 0.36893823400565834\n",
            "training accuracy: 0.895 and validation accuracy: 0.9206\n",
            "iteration 1500 / 2000 :loss 0.30261268008203634\n",
            "training accuracy: 0.93 and validation accuracy: 0.9197\n",
            "iteration 1510 / 2000 :loss 0.310665599848854\n",
            "training accuracy: 0.89 and validation accuracy: 0.9205\n",
            "iteration 1520 / 2000 :loss 0.27518608903454245\n",
            "training accuracy: 0.93 and validation accuracy: 0.9218\n",
            "iteration 1530 / 2000 :loss 0.23129895986496224\n",
            "training accuracy: 0.96 and validation accuracy: 0.9229\n",
            "iteration 1540 / 2000 :loss 0.24866274790992354\n",
            "training accuracy: 0.925 and validation accuracy: 0.9227\n",
            "iteration 1550 / 2000 :loss 0.28605941630870363\n",
            "training accuracy: 0.93 and validation accuracy: 0.9236\n",
            "iteration 1560 / 2000 :loss 0.23984132185875218\n",
            "training accuracy: 0.92 and validation accuracy: 0.9235\n",
            "iteration 1570 / 2000 :loss 0.34696584745944997\n",
            "training accuracy: 0.9 and validation accuracy: 0.9212\n",
            "iteration 1580 / 2000 :loss 0.29097809702164407\n",
            "training accuracy: 0.925 and validation accuracy: 0.9205\n",
            "iteration 1590 / 2000 :loss 0.38225018849930975\n",
            "training accuracy: 0.9 and validation accuracy: 0.9215\n",
            "iteration 1600 / 2000 :loss 0.2959751628404841\n",
            "training accuracy: 0.915 and validation accuracy: 0.923\n",
            "iteration 1610 / 2000 :loss 0.21184708936905494\n",
            "training accuracy: 0.955 and validation accuracy: 0.9222\n",
            "iteration 1620 / 2000 :loss 0.26993874399955176\n",
            "training accuracy: 0.945 and validation accuracy: 0.9209\n",
            "iteration 1630 / 2000 :loss 0.32757401990168344\n",
            "training accuracy: 0.92 and validation accuracy: 0.9237\n",
            "iteration 1640 / 2000 :loss 0.37658284344365067\n",
            "training accuracy: 0.9 and validation accuracy: 0.9251\n",
            "iteration 1650 / 2000 :loss 0.2256984097805114\n",
            "training accuracy: 0.92 and validation accuracy: 0.9223\n",
            "iteration 1660 / 2000 :loss 0.3100990679130669\n",
            "training accuracy: 0.94 and validation accuracy: 0.924\n",
            "iteration 1670 / 2000 :loss 0.28265672740777803\n",
            "training accuracy: 0.92 and validation accuracy: 0.9238\n",
            "iteration 1680 / 2000 :loss 0.27250346456770275\n",
            "training accuracy: 0.92 and validation accuracy: 0.9247\n",
            "iteration 1690 / 2000 :loss 0.24654217220378716\n",
            "training accuracy: 0.935 and validation accuracy: 0.9262\n",
            "iteration 1700 / 2000 :loss 0.2486768274418083\n",
            "training accuracy: 0.935 and validation accuracy: 0.9252\n",
            "iteration 1710 / 2000 :loss 0.22128666989245643\n",
            "training accuracy: 0.95 and validation accuracy: 0.9249\n",
            "iteration 1720 / 2000 :loss 0.41567335752394113\n",
            "training accuracy: 0.87 and validation accuracy: 0.9212\n",
            "iteration 1730 / 2000 :loss 0.245818981575871\n",
            "training accuracy: 0.93 and validation accuracy: 0.9238\n",
            "iteration 1740 / 2000 :loss 0.28746018309012844\n",
            "training accuracy: 0.895 and validation accuracy: 0.9247\n",
            "iteration 1750 / 2000 :loss 0.29365861908204693\n",
            "training accuracy: 0.905 and validation accuracy: 0.9248\n",
            "iteration 1760 / 2000 :loss 0.28813223318265896\n",
            "training accuracy: 0.935 and validation accuracy: 0.9259\n",
            "iteration 1770 / 2000 :loss 0.2498405228737628\n",
            "training accuracy: 0.93 and validation accuracy: 0.926\n",
            "iteration 1780 / 2000 :loss 0.4409465651950268\n",
            "training accuracy: 0.91 and validation accuracy: 0.9251\n",
            "iteration 1790 / 2000 :loss 0.264541624583517\n",
            "training accuracy: 0.92 and validation accuracy: 0.9257\n",
            "iteration 1800 / 2000 :loss 0.29591882898652555\n",
            "training accuracy: 0.915 and validation accuracy: 0.9251\n",
            "iteration 1810 / 2000 :loss 0.32535657663767303\n",
            "training accuracy: 0.93 and validation accuracy: 0.9268\n",
            "iteration 1820 / 2000 :loss 0.4502821496158866\n",
            "training accuracy: 0.875 and validation accuracy: 0.9253\n",
            "iteration 1830 / 2000 :loss 0.2709571150326917\n",
            "training accuracy: 0.925 and validation accuracy: 0.9266\n",
            "iteration 1840 / 2000 :loss 0.2934380136521472\n",
            "training accuracy: 0.915 and validation accuracy: 0.9242\n",
            "iteration 1850 / 2000 :loss 0.2601820960030675\n",
            "training accuracy: 0.935 and validation accuracy: 0.925\n",
            "iteration 1860 / 2000 :loss 0.32771027673190434\n",
            "training accuracy: 0.93 and validation accuracy: 0.9261\n",
            "iteration 1870 / 2000 :loss 0.2816117753960512\n",
            "training accuracy: 0.925 and validation accuracy: 0.9263\n",
            "iteration 1880 / 2000 :loss 0.2760898832197165\n",
            "training accuracy: 0.915 and validation accuracy: 0.9261\n",
            "iteration 1890 / 2000 :loss 0.22630230869237686\n",
            "training accuracy: 0.92 and validation accuracy: 0.9266\n",
            "iteration 1900 / 2000 :loss 0.3091999916510044\n",
            "training accuracy: 0.92 and validation accuracy: 0.9255\n",
            "iteration 1910 / 2000 :loss 0.15107698799032657\n",
            "training accuracy: 0.965 and validation accuracy: 0.9269\n",
            "iteration 1920 / 2000 :loss 0.2711754252205791\n",
            "training accuracy: 0.915 and validation accuracy: 0.9281\n",
            "iteration 1930 / 2000 :loss 0.3293559895118405\n",
            "training accuracy: 0.915 and validation accuracy: 0.9244\n",
            "iteration 1940 / 2000 :loss 0.30069176653979834\n",
            "training accuracy: 0.95 and validation accuracy: 0.9295\n",
            "iteration 1950 / 2000 :loss 0.2543943987808715\n",
            "training accuracy: 0.93 and validation accuracy: 0.9296\n",
            "iteration 1960 / 2000 :loss 0.2469448035615811\n",
            "training accuracy: 0.94 and validation accuracy: 0.928\n",
            "iteration 1970 / 2000 :loss 0.23454710749637175\n",
            "training accuracy: 0.925 and validation accuracy: 0.9284\n",
            "iteration 1980 / 2000 :loss 0.2632015559020795\n",
            "training accuracy: 0.945 and validation accuracy: 0.9266\n",
            "iteration 1990 / 2000 :loss 0.2040512223818366\n",
            "training accuracy: 0.955 and validation accuracy: 0.9254\n",
            "iteration 2000 / 2000 :loss 0.35719262129153556\n",
            "training accuracy: 0.9 and validation accuracy: 0.9269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JQ2A1Wx9lqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f0a8dfb-8c8d-4092-b1f4-73e95e5709ce"
      },
      "source": [
        "parameters, loss_history = TwoLayerNN(0.1, 1000, 200, train_, tr_labels, val, val_labels)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 1000 :loss 5.436974210402086\n",
            "training accuracy: 0.055 and validation accuracy: 0.0764\n",
            "iteration 10 / 1000 :loss 2.2603439681542103\n",
            "training accuracy: 0.3 and validation accuracy: 0.282\n",
            "iteration 20 / 1000 :loss 1.651169990785532\n",
            "training accuracy: 0.485 and validation accuracy: 0.4407\n",
            "iteration 30 / 1000 :loss 1.3789651527713624\n",
            "training accuracy: 0.595 and validation accuracy: 0.5479\n",
            "iteration 40 / 1000 :loss 1.2519428742165593\n",
            "training accuracy: 0.63 and validation accuracy: 0.6188\n",
            "iteration 50 / 1000 :loss 1.0778118434263702\n",
            "training accuracy: 0.645 and validation accuracy: 0.671\n",
            "iteration 60 / 1000 :loss 1.0090354725480137\n",
            "training accuracy: 0.7 and validation accuracy: 0.7064\n",
            "iteration 70 / 1000 :loss 1.0240694963881674\n",
            "training accuracy: 0.665 and validation accuracy: 0.7363\n",
            "iteration 80 / 1000 :loss 0.8827700307649157\n",
            "training accuracy: 0.73 and validation accuracy: 0.7534\n",
            "iteration 90 / 1000 :loss 0.7945107403965119\n",
            "training accuracy: 0.75 and validation accuracy: 0.7653\n",
            "iteration 100 / 1000 :loss 0.9732148571638873\n",
            "training accuracy: 0.76 and validation accuracy: 0.7773\n",
            "iteration 110 / 1000 :loss 0.6262685749424401\n",
            "training accuracy: 0.81 and validation accuracy: 0.7912\n",
            "iteration 120 / 1000 :loss 0.727035754967348\n",
            "training accuracy: 0.79 and validation accuracy: 0.7982\n",
            "iteration 130 / 1000 :loss 0.8009108257115441\n",
            "training accuracy: 0.775 and validation accuracy: 0.7993\n",
            "iteration 140 / 1000 :loss 0.8109730862507162\n",
            "training accuracy: 0.72 and validation accuracy: 0.8112\n",
            "iteration 150 / 1000 :loss 0.5656347891775506\n",
            "training accuracy: 0.815 and validation accuracy: 0.8206\n",
            "iteration 160 / 1000 :loss 0.6232614246986553\n",
            "training accuracy: 0.83 and validation accuracy: 0.8263\n",
            "iteration 170 / 1000 :loss 0.6094511941421501\n",
            "training accuracy: 0.825 and validation accuracy: 0.8287\n",
            "iteration 180 / 1000 :loss 0.5937111354398775\n",
            "training accuracy: 0.855 and validation accuracy: 0.8317\n",
            "iteration 190 / 1000 :loss 0.6161248691477474\n",
            "training accuracy: 0.855 and validation accuracy: 0.8371\n",
            "iteration 200 / 1000 :loss 0.49530490681182326\n",
            "training accuracy: 0.87 and validation accuracy: 0.8443\n",
            "iteration 210 / 1000 :loss 0.510128900628643\n",
            "training accuracy: 0.855 and validation accuracy: 0.8437\n",
            "iteration 220 / 1000 :loss 0.613353180245572\n",
            "training accuracy: 0.82 and validation accuracy: 0.849\n",
            "iteration 230 / 1000 :loss 0.6287674107216366\n",
            "training accuracy: 0.81 and validation accuracy: 0.8509\n",
            "iteration 240 / 1000 :loss 0.5308872076120261\n",
            "training accuracy: 0.845 and validation accuracy: 0.8539\n",
            "iteration 250 / 1000 :loss 0.6141309952388903\n",
            "training accuracy: 0.83 and validation accuracy: 0.8513\n",
            "iteration 260 / 1000 :loss 0.549144276341303\n",
            "training accuracy: 0.83 and validation accuracy: 0.8566\n",
            "iteration 270 / 1000 :loss 0.47852386320947055\n",
            "training accuracy: 0.87 and validation accuracy: 0.8566\n",
            "iteration 280 / 1000 :loss 0.5898053894129438\n",
            "training accuracy: 0.815 and validation accuracy: 0.8568\n",
            "iteration 290 / 1000 :loss 0.4893239806010809\n",
            "training accuracy: 0.865 and validation accuracy: 0.86\n",
            "iteration 300 / 1000 :loss 0.6769371735988611\n",
            "training accuracy: 0.83 and validation accuracy: 0.8636\n",
            "iteration 310 / 1000 :loss 0.36852746974404155\n",
            "training accuracy: 0.91 and validation accuracy: 0.8642\n",
            "iteration 320 / 1000 :loss 0.3971746745701645\n",
            "training accuracy: 0.9 and validation accuracy: 0.8637\n",
            "iteration 330 / 1000 :loss 0.3670648555746141\n",
            "training accuracy: 0.9 and validation accuracy: 0.8657\n",
            "iteration 340 / 1000 :loss 0.5051336613951134\n",
            "training accuracy: 0.855 and validation accuracy: 0.8696\n",
            "iteration 350 / 1000 :loss 0.45450456951372326\n",
            "training accuracy: 0.87 and validation accuracy: 0.8683\n",
            "iteration 360 / 1000 :loss 0.43416191509049074\n",
            "training accuracy: 0.885 and validation accuracy: 0.8735\n",
            "iteration 370 / 1000 :loss 0.5122779850676551\n",
            "training accuracy: 0.865 and validation accuracy: 0.8729\n",
            "iteration 380 / 1000 :loss 0.33396246903973165\n",
            "training accuracy: 0.93 and validation accuracy: 0.8735\n",
            "iteration 390 / 1000 :loss 0.43223204051229125\n",
            "training accuracy: 0.9 and validation accuracy: 0.877\n",
            "iteration 400 / 1000 :loss 0.41233871353492874\n",
            "training accuracy: 0.895 and validation accuracy: 0.8774\n",
            "iteration 410 / 1000 :loss 0.3519245152041588\n",
            "training accuracy: 0.92 and validation accuracy: 0.877\n",
            "iteration 420 / 1000 :loss 0.52114970761693\n",
            "training accuracy: 0.88 and validation accuracy: 0.8777\n",
            "iteration 430 / 1000 :loss 0.44010056337627973\n",
            "training accuracy: 0.875 and validation accuracy: 0.8722\n",
            "iteration 440 / 1000 :loss 0.343210175037409\n",
            "training accuracy: 0.895 and validation accuracy: 0.8783\n",
            "iteration 450 / 1000 :loss 0.3776317908190056\n",
            "training accuracy: 0.895 and validation accuracy: 0.8772\n",
            "iteration 460 / 1000 :loss 0.4050585662656613\n",
            "training accuracy: 0.875 and validation accuracy: 0.8799\n",
            "iteration 470 / 1000 :loss 0.5766868546976478\n",
            "training accuracy: 0.845 and validation accuracy: 0.8799\n",
            "iteration 480 / 1000 :loss 0.4190330566795565\n",
            "training accuracy: 0.865 and validation accuracy: 0.8804\n",
            "iteration 490 / 1000 :loss 0.43545598980758826\n",
            "training accuracy: 0.875 and validation accuracy: 0.8817\n",
            "iteration 500 / 1000 :loss 0.4000423454191629\n",
            "training accuracy: 0.875 and validation accuracy: 0.8826\n",
            "iteration 510 / 1000 :loss 0.44546189086633226\n",
            "training accuracy: 0.885 and validation accuracy: 0.8838\n",
            "iteration 520 / 1000 :loss 0.32107479396564675\n",
            "training accuracy: 0.925 and validation accuracy: 0.8842\n",
            "iteration 530 / 1000 :loss 0.3846510405494526\n",
            "training accuracy: 0.88 and validation accuracy: 0.8862\n",
            "iteration 540 / 1000 :loss 0.3069023642003153\n",
            "training accuracy: 0.92 and validation accuracy: 0.8841\n",
            "iteration 550 / 1000 :loss 0.3414656843094739\n",
            "training accuracy: 0.9 and validation accuracy: 0.8892\n",
            "iteration 560 / 1000 :loss 0.4418267369774147\n",
            "training accuracy: 0.88 and validation accuracy: 0.8882\n",
            "iteration 570 / 1000 :loss 0.4060882438096273\n",
            "training accuracy: 0.875 and validation accuracy: 0.888\n",
            "iteration 580 / 1000 :loss 0.324213148373823\n",
            "training accuracy: 0.91 and validation accuracy: 0.887\n",
            "iteration 590 / 1000 :loss 0.3759981948310742\n",
            "training accuracy: 0.895 and validation accuracy: 0.8864\n",
            "iteration 600 / 1000 :loss 0.3380158418263274\n",
            "training accuracy: 0.91 and validation accuracy: 0.8891\n",
            "iteration 610 / 1000 :loss 0.3268874463528858\n",
            "training accuracy: 0.9 and validation accuracy: 0.888\n",
            "iteration 620 / 1000 :loss 0.37101790516322297\n",
            "training accuracy: 0.905 and validation accuracy: 0.8902\n",
            "iteration 630 / 1000 :loss 0.4536710607461897\n",
            "training accuracy: 0.895 and validation accuracy: 0.8905\n",
            "iteration 640 / 1000 :loss 0.3822160749517647\n",
            "training accuracy: 0.885 and validation accuracy: 0.8899\n",
            "iteration 650 / 1000 :loss 0.45423861115392133\n",
            "training accuracy: 0.89 and validation accuracy: 0.8917\n",
            "iteration 660 / 1000 :loss 0.524225346435646\n",
            "training accuracy: 0.865 and validation accuracy: 0.8932\n",
            "iteration 670 / 1000 :loss 0.3243066948425376\n",
            "training accuracy: 0.92 and validation accuracy: 0.8953\n",
            "iteration 680 / 1000 :loss 0.37731808758242524\n",
            "training accuracy: 0.93 and validation accuracy: 0.8911\n",
            "iteration 690 / 1000 :loss 0.3676972724522543\n",
            "training accuracy: 0.885 and validation accuracy: 0.8947\n",
            "iteration 700 / 1000 :loss 0.3879555486050858\n",
            "training accuracy: 0.885 and validation accuracy: 0.8945\n",
            "iteration 710 / 1000 :loss 0.4369973777046965\n",
            "training accuracy: 0.89 and validation accuracy: 0.8943\n",
            "iteration 720 / 1000 :loss 0.39440523450188936\n",
            "training accuracy: 0.9 and validation accuracy: 0.8964\n",
            "iteration 730 / 1000 :loss 0.3848376936166907\n",
            "training accuracy: 0.885 and validation accuracy: 0.8947\n",
            "iteration 740 / 1000 :loss 0.3003867623819618\n",
            "training accuracy: 0.925 and validation accuracy: 0.8972\n",
            "iteration 750 / 1000 :loss 0.5305661497645053\n",
            "training accuracy: 0.905 and validation accuracy: 0.898\n",
            "iteration 760 / 1000 :loss 0.359648871708232\n",
            "training accuracy: 0.89 and validation accuracy: 0.8948\n",
            "iteration 770 / 1000 :loss 0.3121926541606997\n",
            "training accuracy: 0.915 and validation accuracy: 0.8939\n",
            "iteration 780 / 1000 :loss 0.33637094017611574\n",
            "training accuracy: 0.93 and validation accuracy: 0.8946\n",
            "iteration 790 / 1000 :loss 0.389501894197702\n",
            "training accuracy: 0.88 and validation accuracy: 0.8976\n",
            "iteration 800 / 1000 :loss 0.3359790818874362\n",
            "training accuracy: 0.9 and validation accuracy: 0.8978\n",
            "iteration 810 / 1000 :loss 0.3061637820607324\n",
            "training accuracy: 0.93 and validation accuracy: 0.8976\n",
            "iteration 820 / 1000 :loss 0.28291479139195475\n",
            "training accuracy: 0.925 and validation accuracy: 0.8979\n",
            "iteration 830 / 1000 :loss 0.35535188834756626\n",
            "training accuracy: 0.93 and validation accuracy: 0.8987\n",
            "iteration 840 / 1000 :loss 0.40359815380672337\n",
            "training accuracy: 0.885 and validation accuracy: 0.8988\n",
            "iteration 850 / 1000 :loss 0.3854602529535541\n",
            "training accuracy: 0.87 and validation accuracy: 0.898\n",
            "iteration 860 / 1000 :loss 0.4749551865030879\n",
            "training accuracy: 0.87 and validation accuracy: 0.8988\n",
            "iteration 870 / 1000 :loss 0.24220454065989228\n",
            "training accuracy: 0.935 and validation accuracy: 0.8984\n",
            "iteration 880 / 1000 :loss 0.29621510009131735\n",
            "training accuracy: 0.935 and validation accuracy: 0.9019\n",
            "iteration 890 / 1000 :loss 0.39252333365079195\n",
            "training accuracy: 0.875 and validation accuracy: 0.899\n",
            "iteration 900 / 1000 :loss 0.23329118077389935\n",
            "training accuracy: 0.94 and validation accuracy: 0.902\n",
            "iteration 910 / 1000 :loss 0.3013225868036818\n",
            "training accuracy: 0.92 and validation accuracy: 0.9015\n",
            "iteration 920 / 1000 :loss 0.2987526294448021\n",
            "training accuracy: 0.925 and validation accuracy: 0.9036\n",
            "iteration 930 / 1000 :loss 0.24905302748823246\n",
            "training accuracy: 0.935 and validation accuracy: 0.9021\n",
            "iteration 940 / 1000 :loss 0.3829804054790884\n",
            "training accuracy: 0.89 and validation accuracy: 0.8972\n",
            "iteration 950 / 1000 :loss 0.2636033755883575\n",
            "training accuracy: 0.93 and validation accuracy: 0.9028\n",
            "iteration 960 / 1000 :loss 0.34457360734954967\n",
            "training accuracy: 0.91 and validation accuracy: 0.9037\n",
            "iteration 970 / 1000 :loss 0.2669058244768206\n",
            "training accuracy: 0.945 and validation accuracy: 0.9011\n",
            "iteration 980 / 1000 :loss 0.3606801722423537\n",
            "training accuracy: 0.895 and validation accuracy: 0.9028\n",
            "iteration 990 / 1000 :loss 0.46603298908890234\n",
            "training accuracy: 0.88 and validation accuracy: 0.9037\n",
            "iteration 1000 / 1000 :loss 0.34164185302356015\n",
            "training accuracy: 0.895 and validation accuracy: 0.9041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y729JQy9lqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3be024c0-149b-4a98-86be-799fa01f2a6c"
      },
      "source": [
        "# plot the loss to see how the loss varied over iterations, generated with learning rate = 0.1, iterations=2000\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.title('Training Loss history')\n",
        "plt.show()\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvurG86T9lqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading and pre=processing test data\n",
        "t_inputs, tlabels = load_images_with_labels(type_of_data='testing')\n",
        "tinputs = t_inputs.reshape(10000, 784)\n",
        "tinputs = np.float32(tinputs)\n",
        "tinputs /= np.max(tinputs,axis=1).reshape(-1, 1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyJO0P99lqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10ea9f59-403c-4f86-8364-780a588068d6"
      },
      "source": [
        "# Calculate the accuracy on test data using trained weights and biases\n",
        "pred = predict(tinputs, parameters)\n",
        "test_acc = (pred == tlabels).mean()\n",
        "print(test_acc)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQjF7n939lqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f4e57ae8-06f0-41cb-db0c-3d5e52e6cf5a"
      },
      "source": [
        "# Predict the labels for n images for visual representation\n",
        "n = 5 # number of images to predict\n",
        "idx = np.random.choice(10000, n, replace=False) # select n random images from test data\n",
        "labl=[]\n",
        "# View first n examples\n",
        "fig, ax = plt.subplots(1,n)\n",
        "for i, val in enumerate(idx):\n",
        "    ax[i].imshow(t_inputs[val], cmap=mpl.cm.Greys)\n",
        "    ax[i].set_title(tlabels[val])\n",
        "    labl.append(pred[val])\n",
        "plt.show()\n",
        "\n",
        "print('Corresponding predictions of labels for each of the above images: '+ str(labl))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Corresponding predictions of labels for each of the above images: [2, 3, 7, 7, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}